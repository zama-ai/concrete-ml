{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfccd8e6",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-2 on Encrypted Data with LoRA and Concrete ML\n",
    "\n",
    "In this notebook, we perform fine-tuning of a GPT-2 model using LoRA and Concrete ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca73e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e78e535d7b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from utils_lora import generate_and_print, print_weights_and_size\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "from concrete.ml.torch.lora import LoraTraining, get_remote_names\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b965a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2337a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a new type of energy storage that is designed to be used in a variety of applications. It is used to store energy in\n"
     ]
    }
   ],
   "source": [
    "generate_and_print(\n",
    "    prompt=\"What is FHE?\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20564b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "# target_modules can be set to \"all-linear\"\n",
    "# to target all modules. By default only the\n",
    "# c_attn projection are fine-tuned with lora.\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac49f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers detected in the model.\n"
     ]
    }
   ],
   "source": [
    "# Set up LoRA training\n",
    "lora_training = LoraTraining(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10d71e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344f0292b9084b36aa03931093f09314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare dataset for fine-tuning\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "# Read lines from the file\n",
    "with open(\"data_finetune/what_is_fhe.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Remove empty lines and strip whitespace\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Group lines into question-answer pairs\n",
    "examples = []\n",
    "for i in range(0, len(lines) - 1, 2):\n",
    "    question = lines[i]\n",
    "    answer = lines[i + 1]\n",
    "    examples.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "# Create a Dataset object from the list of examples\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    attention_masks_list = []\n",
    "    for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        # Tokenize question and answer separately\n",
    "        question_encoding = tokenizer(\n",
    "            question, add_special_tokens=False, truncation=True, max_length=BLOCK_SIZE // 2\n",
    "        )\n",
    "        answer_encoding = tokenizer(\n",
    "            answer, add_special_tokens=False, truncation=True, max_length=BLOCK_SIZE // 2 - 1\n",
    "        )\n",
    "\n",
    "        # Build input_ids\n",
    "        input_ids = (\n",
    "            question_encoding[\"input_ids\"]\n",
    "            + [tokenizer.eos_token_id]\n",
    "            + answer_encoding[\"input_ids\"]\n",
    "            + [tokenizer.eos_token_id]\n",
    "        )\n",
    "\n",
    "        # Build labels: -100 for question tokens and eos token after question\n",
    "        labels = (\n",
    "            [-100] * len(question_encoding[\"input_ids\"])\n",
    "            + [-100]  # For the eos token after question\n",
    "            + answer_encoding[\"input_ids\"]\n",
    "            + [tokenizer.eos_token_id]\n",
    "        )\n",
    "\n",
    "        # Create attention mask: 1 for real tokens, 0 for padding\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Pad/truncate to BLOCK_SIZE\n",
    "        padding_length = BLOCK_SIZE - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            labels += [-100] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "        else:\n",
    "            input_ids = input_ids[:BLOCK_SIZE]\n",
    "            labels = labels[:BLOCK_SIZE]\n",
    "            attention_mask = attention_mask[:BLOCK_SIZE]\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(labels)\n",
    "        attention_masks_list.append(attention_mask)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"labels\": labels_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the tokenization\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"question\", \"answer\"]\n",
    ")\n",
    "\n",
    "# Since we've already handled padding and labels, we can use a custom data collator\n",
    "\n",
    "\n",
    "def data_collator(features):\n",
    "    batch = {}\n",
    "    batch[\"input_ids\"] = torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long)\n",
    "    batch[\"labels\"] = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "    batch[\"attention_mask\"] = torch.tensor(\n",
    "        [f[\"attention_mask\"] for f in features], dtype=torch.long\n",
    "    )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a01acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "EPOCHS = 20\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=2e-3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def causal_lm_loss(logits, labels, ignore_index=-100):\n",
    "    # Shift logits and labels for next-token prediction\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Flatten the tensors\n",
    "    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "\n",
    "    # Compute the loss, ignoring padding tokens\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        shift_logits, shift_labels, ignore_index=ignore_index, reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8864b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = len_dataloader // training_args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "max_steps = math.ceil(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
    "\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "lr_scheduler = trainer.lr_scheduler\n",
    "optimizer = trainer.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2094a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the remote modules (layers to be converted to FHE)\n",
    "remote_names = get_remote_names(lora_training, include_embedding_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21298ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HybridFHEModel with the specified remote modules\n",
    "hybrid_model = HybridFHEModel(lora_training, module_names=remote_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ec41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data for calibration\n",
    "input_tensor = torch.randint(\n",
    "    0, tokenizer.vocab_size, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE), dtype=torch.long\n",
    ")\n",
    "label_tensor = torch.randint(\n",
    "    0, tokenizer.vocab_size, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE), dtype=torch.long\n",
    ")\n",
    "attention_mask = torch.ones((PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE), dtype=torch.long)\n",
    "\n",
    "inputset = {\"input_ids\": input_tensor, \"attention_mask\": attention_mask, \"labels\": label_tensor}\n",
    "\n",
    "# inputset = (input_tensor, label_tensor, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20dfe2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:12<00:00,  7.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calibrate and compile the model\n",
    "lora_training.toggle_calibrate(enable=True)\n",
    "hybrid_model.compile_model(inputset, n_bits=16)\n",
    "lora_training.toggle_calibrate(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e450e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(\n",
    "    hybrid_model, train_dataloader, training_args, tokenizer, fhe=\"disable\"\n",
    "):  # pylint: disable=too-many-locals\n",
    "    device = \"cpu\"\n",
    "    hybrid_model.model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    peft_model.train()\n",
    "    total_epochs = int(training_args.num_train_epochs)\n",
    "    epoch_pbar = tqdm(total=total_epochs, desc=\"Training Progress\", position=0)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Generate text before the first epoch\n",
    "    print(\"Generating text before the first epoch:\\n\")\n",
    "    prompt = \"What is FHE?\"\n",
    "    hybrid_model.set_fhe_mode(\"disable\")\n",
    "    generate_and_print(prompt, peft_model, tokenizer, SEED)\n",
    "    hybrid_model.set_fhe_mode(fhe)\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        grad_norms = []\n",
    "\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            total_batched_samples += 1\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss, grad_norm = hybrid_model(batch, fhe=fhe)\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Learning rate scheduler step\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if grad_norm is not None:\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Get last grad norm\n",
    "        current_grad_norm = grad_norms[-1] if grad_norms else None\n",
    "\n",
    "        # Store the total loss for this epoch\n",
    "        epoch_losses.append(total_loss)\n",
    "\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, \"\n",
    "            f\"Loss: {total_loss:.4f}, grad norm: {current_grad_norm}, lr: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        # Generate text after each epoch\n",
    "        prompt = \"What is FHE?\"\n",
    "        hybrid_model.set_fhe_mode(\"disable\")\n",
    "        generate_and_print(prompt, peft_model, tokenizer, SEED)\n",
    "        hybrid_model.set_fhe_mode(fhe)\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 50)  # Separator for readability\n",
    "        epoch_pbar.update(1)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if training_args.output_dir is not None:\n",
    "        save_path = f\"{training_args.output_dir}/checkpoint-{epoch + 1}\"\n",
    "        peft_model.save_pretrained(save_path)\n",
    "\n",
    "    epoch_pbar.close()\n",
    "\n",
    "    # Plot the loss evolution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, total_epochs + 1), epoch_losses, marker=\"o\")\n",
    "    plt.title(\"Loss Evolution During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Total Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ca82a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text before the first epoch:\n",
      "\n",
      "What is FHE? FHE is a new type of energy that is used to generate electricity. It is the most energy-efficient form of electricity, and it\n",
      "Epoch 1/20, Loss: 31.7650, grad norm: None, lr: 0.0018000000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 1/20 [00:22<07:07, 22.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a non-invasive, noninvasible, and nonflammable, biocompatible, anti-inflammatory\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 2/20, Loss: 26.5012, grad norm: None, lr: 0.0019058823529411763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 2/20 [00:29<04:02, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a simple, fast, efficient, and is used to encrypt and decrypt data. It is also encrypts data in encrypted data\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 3/20, Loss: 23.3150, grad norm: None, lr: 0.0018000000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 3/20 [00:36<02:57, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a simple, fast, and efficient way to perform computations on Fully-Fully Homomorphic Encryption (F-\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 4/20, Loss: 20.6332, grad norm: None, lr: 0.0016941176470588236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 4/20 [00:43<02:24,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a non-static structure that can only be used by nonstructural operators such as functions or functions. It is used to\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 5/20, Loss: 19.3935, grad norm: None, lr: 0.001588235294117647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 5/20 [00:50<02:03,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a non-trivial ciphertext-free way to store and display encrypted data in plaintext. Fhe is an\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 6/20, Loss: 17.2984, grad norm: None, lr: 0.0014823529411764707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 6/20 [00:57<01:49,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a type of Fully Homomorphic Encryption (FHO) that can be used to perform computations on encrypted data. F\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 7/20, Loss: 15.0680, grad norm: None, lr: 0.001376470588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 7/20 [01:03<01:37,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a type of Fully Homomorphic Encryption (FHO) that allows computations on encrypted data without revealing the underlying underlying data\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 8/20, Loss: 14.5426, grad norm: None, lr: 0.0012705882352941175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 8/20 [01:10<01:27,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a fast-growing encryption algorithm that allows for efficient computations without requiring any additional hardware hardware. It is designed to support both\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 9/20, Loss: 13.4604, grad norm: None, lr: 0.0011647058823529412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 9/20 [01:17<01:18,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a fast, low-latency, encrypted, superfast, high-performance computing model for machine-learning tasks. It enables\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 10/20, Loss: 12.7730, grad norm: None, lr: 0.0010588235294117648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 10/20 [01:24<01:11,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a type of fast-forward-decryption algorithm that uses a combination of encryption and decryption. Fhe is implemented using\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 11/20, Loss: 11.2115, grad norm: None, lr: 0.0009529411764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 11/20 [01:31<01:03,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? F(F) is a subset of Fully Homomorphic Encryption (FHE) that can be encrypted using GHE principles. FHe is particularly\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 12/20, Loss: 10.3693, grad norm: None, lr: 0.0008470588235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 12/20 [01:38<00:55,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? F HE is a type of encryption that allows for arbitrary numbers of values to be encrypted. This is particularly useful for sensitive information, where sensitive data can\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 13/20, Loss: 9.8846, grad norm: None, lr: 0.0007411764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 13/20 [01:45<00:48,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FITTING Fully Fully Fully Homomorphic Encryption (FITT) by using encrypted data to encrypt it. Fittings are performed using a\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 14/20, Loss: 9.1691, grad norm: None, lr: 0.0006352941176470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 14/20 [01:52<00:41,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a Fully Homomorphic Encryption (FHO) that allows computations on encrypted data without any additional processing or computments on\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 15/20, Loss: 8.7973, grad norm: None, lr: 0.0005294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 15/20 [01:59<00:34,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a subset of Fully Homomorphic Encryption (FHO) that allows computations on encrypted data without exposing the underlying ciphertext\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 16/20, Loss: 8.3489, grad norm: None, lr: 0.0004235294117647059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 16/20 [02:06<00:27,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a fast-growing encryption scheme that supports both single-cooperant encryption and encrypted-fHE (though it can be\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 17/20, Loss: 7.5543, grad norm: None, lr: 0.0003176470588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 17/20 [02:13<00:20,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a subset of Fully Homomorphic (FITT) that supports encrypted data-images on encrypted strings. Unlike traditional encryption methods,\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 18/20, Loss: 7.2944, grad norm: None, lr: 0.00021176470588235295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 18/20 [02:20<00:14,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a subset of Fully Homomorphic Encryption (FHO) that allows secure data access to arbitrary sensitive data without encryption. F\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 19/20, Loss: 7.2749, grad norm: None, lr: 0.00010588235294117647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 19/20 [02:28<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a subset of Fully Homomorphic Encryption (FHO) that allows secure data access to arbitrary cipherments without ever decrypting\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 20/20, Loss: 6.9284, grad norm: None, lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [02:36<00:00,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a subset of Fully Homomorphic Encryption (FHO) that allows secure data access to arbitrary cipherments without ever decrypting\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [02:36<00:00,  7.83s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Avoid parallelism error from HuggingFace during training\n",
    "tokenizer.parallelism = False\n",
    "\n",
    "# Train the model using FHE simulation\n",
    "train_custom_model(hybrid_model, train_dataloader, training_args, tokenizer, fhe=\"disable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d448c8",
   "metadata": {},
   "source": [
    "Note that our goal is to showcase the use of FHE for encrypted fine-tuning. The dataset consists of 68 examples and a total of 2,386 tokens, which is relatively small. Despite its limited size, which offers little support for the model's learning process, it still manages to produce interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd666f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fine-tuned model\n",
    "fine_tuned_model = hybrid_model.model.inference_model\n",
    "\n",
    "# Set FHE mode to disable for text generation\n",
    "hybrid_model.set_fhe_mode(\"execute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e91ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 5, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 5, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.5.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.6.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.7.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.8.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.9.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.10.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.11.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_attn.base_layer\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.0.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.1.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.2.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.3.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_fc.forward_module\n",
      "(1, 1, 3072)\n",
      "inference_model.base_model.model.transformer.h.4.mlp.c_proj.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_attn.base_layer.forward_module\n",
      "(1, 1, 768)\n",
      "inference_model.base_model.model.transformer.h.5.attn.c_proj.forward_module\n",
      "(1, 1, 768)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m fine_tuned_model\u001b[38;5;241m.\u001b[39menable_adapter_layers()\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho invented FHE?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgenerate_and_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tuned_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/use_case_examples/lora_finetuning/utils_lora.py:64\u001b[0m, in \u001b[0;36mgenerate_and_print\u001b[0;34m(prompt, model, tokenizer, seed, max_new_tokens)\u001b[0m\n\u001b[1;32m     61\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Decode the generated text\u001b[39;00m\n\u001b[1;32m     78\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/peft/peft_model.py:1491\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1490\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1491\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1272\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1272\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1133\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1122\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1123\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         output_attentions,\n\u001b[1;32m   1131\u001b[0m     )\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:652\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    650\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 652\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:575\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/src/concrete/ml/torch/hybrid_backprop_linear.py:78\u001b[0m, in \u001b[0;36mCustomLinear.forward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of the custom linear module.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m        The output tensor after applying the custom linear module.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mForwardBackwardModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/src/concrete/ml/torch/hybrid_backprop_linear.py:98\u001b[0m, in \u001b[0;36mForwardBackwardModule.forward\u001b[0;34m(ctx, input_tensor, forward_module, backward_module)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of the custom autograd function.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    The output tensor after applying the forward pass.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mbackward_module \u001b[38;5;241m=\u001b[39m backward_module\n\u001b[0;32m---> 98\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/src/concrete/ml/torch/hybrid_model.py:259\u001b[0m, in \u001b[0;36mRemoteModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivate_q_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# Delegate to the optimized GLWE executor\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprivate_q_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfhe_local_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/src/concrete/ml/quantization/linear_op_glwe_backend.py:161\u001b[0m, in \u001b[0;36mGLWELinearLayerExecutor.forward\u001b[0;34m(self, x, q_module, fhe)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, q_x_sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(q_x):\n\u001b[1;32m    156\u001b[0m     ciphertext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhext\u001b[38;5;241m.\u001b[39mencrypt_matrix(  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         pkey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivate_key, crypto_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglwe_crypto_params, data\u001b[38;5;241m=\u001b[39mq_x_sample\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     encrypted_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhext\u001b[38;5;241m.\u001b[39mmatrix_multiplication(  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         encrypted_matrix\u001b[38;5;241m=\u001b[39mciphertext,\n\u001b[0;32m--> 161\u001b[0m         data\u001b[38;5;241m=\u001b[39mq_weight\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39muint64),\n\u001b[1;32m    162\u001b[0m         compression_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression_key,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     q_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhext\u001b[38;5;241m.\u001b[39mdecrypt_matrix(  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         encrypted_result,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivate_key,\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglwe_crypto_params,\n\u001b[1;32m    168\u001b[0m         num_valid_glwe_values_in_last_ciphertext,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     q_result \u001b[38;5;241m=\u001b[39m q_result\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39mint64)\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/.venv/lib/python3.11/site-packages/concrete_ml_extensions/__init__.py:17\u001b[0m, in \u001b[0;36mmatrix_multiplication\u001b[0;34m(encrypted_matrix, data, compression_key)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatrix_multiplication\u001b[39m(encrypted_matrix, data, compression_key):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_cuda_enabled() \u001b[38;5;129;01mand\u001b[39;00m is_cuda_available():\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cuda_matrix_multiplication(encrypted_matrix, data, compression_key)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cpu_matrix_multiplication(encrypted_matrix, data, compression_key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inference using the fine-tuned model with LoRA weights\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "fine_tuned_model.enable_adapter_layers()\n",
    "prompt = \"who invented FHE?\"\n",
    "generate_and_print(prompt, fine_tuned_model, tokenizer, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FHE is a type of the term \"f\" that is the name of a group of people who are not part of that group.\n"
     ]
    }
   ],
   "source": [
    "# Original inference without LoRA weights\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "peft_model.disable_adapter_layers()\n",
    "\n",
    "prompt = \"What is FHE?\"\n",
    "generate_and_print(prompt, peft_model, tokenizer, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c97425ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 124734720\n",
      "Total number of LoRA weights: 294912\n"
     ]
    }
   ],
   "source": [
    "peft_model.enable_adapter_layers()\n",
    "\n",
    "# Print weights and model size\n",
    "total_weights_size = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31367ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "GLWE backend deployment is not yet supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_dir() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(path\u001b[38;5;241m.\u001b[39miterdir()):\n\u001b[1;32m      6\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(path)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_and_clear_private_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Private/Work/concrete-ml/src/concrete/ml/torch/hybrid_model.py:679\u001b[0m, in \u001b[0;36mHybridFHEModel.save_and_clear_private_info\u001b[0;34m(self, path, via_mlir)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/4672\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;66;03m# GLWE backend deployment is not yet supported\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLWE backend deployment is not yet supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    681\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[1;32m    682\u001b[0m path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: GLWE backend deployment is not yet supported"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "path = Path(\"deployment/gpt2_lora_finetuned\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if path.is_dir() and any(path.iterdir()):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "hybrid_model.save_and_clear_private_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1dda636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 39717120\n",
      "Total number of LoRA weights: 294912\n"
     ]
    }
   ],
   "source": [
    "# Print weights and size after saving\n",
    "total_weights_size_private = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "506ad2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total weights removed: 68.16 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the percentage of weights removed\n",
    "percentage_removed = (total_weights_size - total_weights_size_private) / total_weights_size * 100\n",
    "print(f\"Total weights removed: {percentage_removed:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "465cb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Around 95% of the remaining weights are from the embedding layers (wpe and wte)\n",
    "# as well as the final lm_head layer."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
