{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfccd8e6",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-2 on Encrypted Data with LoRA and Concrete-ML\n",
    "\n",
    "In this notebook, we peform fine-tuning of a GPT-2 model using LoRA and Concrete-ML. This allows us to fine-tune a model in a privacy-preserving manner.\n",
    "\n",
    "LoRA weight can be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca73e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from utils_lora import generate_text, get_remote_names, print_weights_and_size, replace_conv1d\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "from concrete.ml.torch.lora import LoraTraining\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b965a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the pre-trained model\n",
    "prompt = \"What is FHE?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20564b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0472e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Conv1D layers with CustomConv1D for FHE compatibility\n",
    "# Skip the first Conv1D layer (module_index_to_skip=0)\n",
    "replace_conv1d(peft_model, module_index_to_skip=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac49f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LoRA training\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "lora_training = LoraTraining(peft_model, GRADIENT_ACCUMULATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for fine-tuning\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data_finetune/what_is_fhe.txt\",\n",
    "    block_size=BLOCK_SIZE,\n",
    "    cache_dir=\"cache_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "EPOCHS = 100\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=5e-4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=0,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8864b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = len_dataloader // training_args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "max_steps = math.ceil(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
    "\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "lora_training.update_training_parameters(trainer.optimizer, trainer.lr_scheduler, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2094a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the remote modules (layers to be converted to FHE)\n",
    "remote_names = get_remote_names(lora_training, include_embedding_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21298ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HybridFHEModel with the specified remote modules\n",
    "hybrid_model = HybridFHEModel(lora_training, module_names=remote_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data for calibration\n",
    "input_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "label_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "\n",
    "inputset = (input_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfe2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate and compile the model\n",
    "hybrid_model.model.toggle_calibrate(enable=True)\n",
    "hybrid_model.compile_model(inputset, n_bits=16)\n",
    "hybrid_model.model.toggle_calibrate(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e450e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(\n",
    "    hybrid_model, train_dataloader, training_args, fhe=\"disable\"\n",
    "):  # pylint: disable=too-many-locals\n",
    "    device = \"cpu\"\n",
    "    hybrid_model.model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    hybrid_model.model.inference_model.train()\n",
    "\n",
    "    total_epochs = int(training_args.num_train_epochs)\n",
    "    epoch_pbar = tqdm(total=total_epochs, desc=\"Training Progress\", position=0)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    epoch_losses = []  # List to store the loss for each epoch\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        grad_norms = []\n",
    "\n",
    "        steps_in_epoch = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Gradient accumulation\n",
    "            is_within_accumulation_steps = (\n",
    "                steps_in_epoch <= training_args.gradient_accumulation_steps\n",
    "            )\n",
    "            is_last_step_in_epoch = (step + 1) == steps_in_epoch\n",
    "\n",
    "            is_last_batch_step = is_within_accumulation_steps and is_last_step_in_epoch\n",
    "\n",
    "            mod_total_batched_samples = (\n",
    "                total_batched_samples % training_args.gradient_accumulation_steps\n",
    "            )\n",
    "            accumulate_gradients = mod_total_batched_samples == 0\n",
    "\n",
    "            run_optimizer = is_last_batch_step or accumulate_gradients\n",
    "\n",
    "            hybrid_model.model.toggle_run_optimizer(enable=run_optimizer)\n",
    "\n",
    "            loss, grad_norm = hybrid_model((batch[\"input_ids\"], batch[\"labels\"]), fhe=fhe)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if grad_norm is not None:\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = hybrid_model.model.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Get last grad norm\n",
    "        current_grad_norm = grad_norms[-1] if grad_norms else None\n",
    "\n",
    "        # Store the total loss for this epoch\n",
    "        epoch_losses.append(total_loss)\n",
    "\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, \"\n",
    "            f\"Loss: {total_loss:.4f}, grad norm: {current_grad_norm}, lr: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if training_args.output_dir is not None:\n",
    "        save_path = f\"{training_args.output_dir}/checkpoint-{epoch + 1}\"\n",
    "        hybrid_model.model.inference_model.save_pretrained(save_path)\n",
    "\n",
    "    epoch_pbar.close()\n",
    "\n",
    "    # Plot the loss evolution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, total_epochs + 1), epoch_losses, marker=\"o\")\n",
    "    plt.title(\"Loss Evolution During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Total Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca82a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid parallelism error from HuggingFace during training\n",
    "tokenizer.parallelism = False\n",
    "\n",
    "# Train the model using FHE simulation\n",
    "train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"simulate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd666f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fine-tuned model\n",
    "fine_tuned_model = hybrid_model.model.inference_model\n",
    "\n",
    "# Set FHE mode to disable for text generation\n",
    "hybrid_model.set_fhe_mode(\"disable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using the fine-tuned model with LoRA weights\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "prompt = \"What is FHE?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original inference without LoRA weights\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "peft_model.disable_adapter_layers()\n",
    "\n",
    "prompt = \"What is FHE?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)\n",
    "\n",
    "peft_model.enable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97425ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights and model size\n",
    "total_weights_size = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31367ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "path = Path(\"deployment/gpt2_lora_finetuned\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if path.is_dir() and any(path.iterdir()):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "hybrid_model.save_and_clear_private_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dda636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights and size after saving\n",
    "total_weights_size_private = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the percentage of weights removed\n",
    "percentage_removed = (total_weights_size - total_weights_size_private) / total_weights_size * 100\n",
    "print(f\"Total weights removed: {percentage_removed:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Around 95% of the remaining weights are from the embedding layers (wpe and wte)\n",
    "# as well as the final lm_head layer."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
