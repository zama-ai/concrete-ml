{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fc3f67",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA with LoRA for Math Word Problems\n",
    "\n",
    "This notebook demonstrates how to fine-tune the LLaMA-3.2-1B model using LoRA (Low-Rank Adaptation) on the Orca Math Word Problems dataset. We support multiple modes (Torch, 8-bit, 16-bit) and include evaluation metrics like perplexity.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a5e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from utils_lora import generate_and_print\n",
    "\n",
    "from concrete.ml.torch.lora import LoraTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db3207",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Define the training mode and other parameters here. Modify these values as needed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf52256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode: 'torch', '7bit', or '16bit'\n",
    "MODE = \"7bit\"\n",
    "\n",
    "# Number of steps between evaluations\n",
    "EVAL_STEPS = 100\n",
    "\n",
    "# Set to True to force CPU\n",
    "FORCE_CPU = False\n",
    "\n",
    "# File paths based on mode\n",
    "mode_str = MODE\n",
    "EVAL_RESPONSES_FILE = f\"eval_generated_responses_{mode_str}.txt\"\n",
    "TRAIN_LOG_FILE = f\"training_log_{mode_str}.txt\"\n",
    "SAVE_PATH = Path(f\"deployment/llama_lora_finetuned_{mode_str}\")\n",
    "\n",
    "print(f\"Mode: {MODE}, Eval Steps: {EVAL_STEPS}, Force CPU: {FORCE_CPU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7aca0",
   "metadata": {},
   "source": [
    "## Device and Seed Configuration\n",
    "\n",
    "Set up the device (CPU/GPU/MPS) and random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83416a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if FORCE_CPU:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The random seed to use\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "DEVICE = get_device()\n",
    "set_seed(SEED)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a064520",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the LLaMA model and tokenizer, and test the base model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab290ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "PROMPT = \"When you multiply a number by 7, it becomes 98. What is that number?\\n\"\n",
    "_ = generate_and_print(PROMPT, model, tokenizer, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855c901",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Set up LoRA parameters and apply them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e83963",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293bc07",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Load the dataset, filter by length, and preprocess for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "raw_dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train\")\n",
    "\n",
    "\n",
    "def length_filter(example):\n",
    "    q_len = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    a_len = len(tokenizer(example[\"answer\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    return (q_len + a_len + 1) <= MAX_LENGTH\n",
    "\n",
    "\n",
    "filtered_dataset = raw_dataset.filter(length_filter)\n",
    "\n",
    "\n",
    "def get_lengths(example):\n",
    "    q_len = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    a_len = len(tokenizer(example[\"answer\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    total_len = q_len + a_len + 1\n",
    "    return {\"q_len\": q_len, \"a_len\": a_len, \"total_len\": total_len}\n",
    "\n",
    "\n",
    "lengths = filtered_dataset.map(get_lengths)\n",
    "q_lengths = [x[\"q_len\"] for x in lengths]\n",
    "a_lengths = [x[\"a_len\"] for x in lengths]\n",
    "total_lengths = [x[\"total_len\"] for x in lengths]\n",
    "\n",
    "print(\"\\nLength Distribution Statistics:\")\n",
    "print(f\"Original dataset size: {len(raw_dataset):,}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset):,}\")\n",
    "print(f\"Percentage kept: {100 * len(filtered_dataset)/len(raw_dataset):.1f}%\\n\")\n",
    "print(\"Question lengths: \")\n",
    "print(f\"  Min: {min(q_lengths)}, Max: {max(q_lengths)}\")\n",
    "print(f\"  Mean: {sum(q_lengths)/len(q_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(q_lengths)[len(q_lengths)//2]}\")\n",
    "print(\"\\nAnswer lengths:\")\n",
    "print(f\"  Min: {min(a_lengths)}, Max: {max(a_lengths)}\")\n",
    "print(f\"  Mean: {sum(a_lengths)/len(a_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(a_lengths)[len(a_lengths)//2]}\")\n",
    "print(\"\\nTotal lengths (including newline):\")\n",
    "print(f\"  Min: {min(total_lengths)}, Max: {max(total_lengths)}\")\n",
    "print(f\"  Mean: {sum(total_lengths)/len(total_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(total_lengths)[len(total_lengths)//2]}\\n\")\n",
    "\n",
    "\n",
    "def process_example(example):\n",
    "    \"\"\"Tokenize a question-answer pair and prepare labels for training.\n",
    "\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'answer' strings\n",
    "    Returns:\n",
    "        dict: Processed tokens with masked labels for the question portion\n",
    "    \"\"\"\n",
    "    question = example[\"question\"].strip()\n",
    "    answer = example[\"answer\"].strip()\n",
    "    tokens = tokenizer(\n",
    "        question + \"\\n\" + answer,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    question_length = len(tokenizer(question, add_special_tokens=False)[\"input_ids\"]) + 1\n",
    "    labels = tokens[\"input_ids\"].copy()\n",
    "    for i in range(question_length):\n",
    "        if i < len(labels):\n",
    "            labels[i] = -100\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_dataset = filtered_dataset.map(\n",
    "    process_example,\n",
    "    batched=False,\n",
    "    remove_columns=filtered_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized = tokenized_dataset.train_test_split(test_size=0.05, seed=SEED, shuffle=True)\n",
    "train_dataset, test_dataset = tokenized[\"train\"], tokenized[\"test\"]\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b530ff",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "Define a custom data collator to handle padding while preserving label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b120a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        inputs, attention_masks, labels = [], [], []\n",
    "        max_length = max(len(example[\"input_ids\"]) for example in examples)\n",
    "        for example in examples:\n",
    "            inputs.append(example[\"input_ids\"])\n",
    "            attention_masks.append(example[\"attention_mask\"])\n",
    "            labels.append(example[\"labels\"])\n",
    "\n",
    "        def pad(sequences, value):\n",
    "            return [x + [value] * (max_length - len(x)) for x in sequences]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(pad(inputs, self.pad_id)),\n",
    "            \"attention_mask\": torch.tensor(pad(attention_masks, 0)),\n",
    "            \"labels\": torch.tensor(pad(labels, -100)),\n",
    "        }\n",
    "\n",
    "\n",
    "collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf397552",
   "metadata": {},
   "source": [
    "## Training Arguments\n",
    "\n",
    "Configure the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db068dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a8346",
   "metadata": {},
   "source": [
    "## Loss and Evaluation Metrics\n",
    "\n",
    "Define the loss function and evaluation metric (perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6eb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_lm_loss(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    return F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=-100,\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "\n",
    "\n",
    "def metric_fn(model, dataloader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    total_loss, total_tokens, results = 0.0, 0, []\n",
    "    response = generate_and_print(PROMPT, model, tokenizer, seed=SEED)\n",
    "    if response:\n",
    "        results.append({\"prompt\": PROMPT, \"response\": response})\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            batch_labels = batch[\"labels\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "            valid = batch_labels[..., 1:] != -100\n",
    "            loss = F.cross_entropy(\n",
    "                outputs[..., :-1, :].contiguous().view(-1, outputs.size(-1)),\n",
    "                batch_labels[..., 1:].contiguous().view(-1),\n",
    "                ignore_index=-100,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += valid.sum().item()\n",
    "    perplexity = math.exp(total_loss / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "    with open(EVAL_RESPONSES_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Perplexity: {perplexity:.2f}\\n\")\n",
    "        for i, r in enumerate(results):\n",
    "            f.write(\n",
    "                f\"== Generation {i+1} ==\\nPrompt:\\n{r['prompt']}\\n\\nResponse:\\n{r['response']}\\n\"\n",
    "            )\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d167837",
   "metadata": {},
   "source": [
    "## Trainer Setup\n",
    "\n",
    "Initialize the Hugging Face Trainer to set up the optimizer and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8e7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "train_dl = hf_trainer.get_train_dataloader()\n",
    "hf_trainer.create_optimizer_and_scheduler(len(train_dl) * training_args.num_train_epochs)\n",
    "optimizer, lr_scheduler = hf_trainer.optimizer, hf_trainer.lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c048d",
   "metadata": {},
   "source": [
    "## Calibration and Evaluation Data\n",
    "\n",
    "Prepare dummy calibration data and the evaluation DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "face520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputset():\n",
    "    return {\n",
    "        \"input_ids\": torch.randint(0, tokenizer.vocab_size, (4, MAX_LENGTH)),\n",
    "        \"attention_mask\": torch.ones((4, MAX_LENGTH), dtype=torch.long),\n",
    "        \"labels\": torch.randint(0, tokenizer.vocab_size, (4, MAX_LENGTH)),\n",
    "    }\n",
    "\n",
    "\n",
    "eval_dl = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afbe6e",
   "metadata": {},
   "source": [
    "## Model Initialization Function\n",
    "\n",
    "Add a function to initialize models with consistent weights for our comparison experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de029b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(seed=SEED):\n",
    "    \"\"\"Initialize the model with consistent weights.\"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.01,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=\"all-linear\",\n",
    "    )\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model = peft_model.to(DEVICE)\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def setup_trainer(model, steps=EVAL_STEPS):\n",
    "    \"\"\"Set up the trainer, optimizer and scheduler.\"\"\"\n",
    "    hf_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    train_dl = hf_trainer.get_train_dataloader()\n",
    "    hf_trainer.create_optimizer_and_scheduler(len(train_dl) * training_args.num_train_epochs)\n",
    "    optimizer, lr_scheduler = hf_trainer.optimizer, hf_trainer.lr_scheduler\n",
    "\n",
    "    lora_trainer = LoraTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=causal_lm_loss,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        training_args=vars(training_args),\n",
    "        n_layers_to_skip_for_backprop=3,\n",
    "        eval_loader=eval_dl,\n",
    "        eval_metric_fn=metric_fn,\n",
    "        logging_steps=1,\n",
    "        eval_steps=steps,\n",
    "        train_log_path=TRAIN_LOG_FILE,\n",
    "    )\n",
    "\n",
    "    if MODE != \"torch\":\n",
    "        bits = 7 if MODE == \"7bit\" else 16\n",
    "        lora_trainer.compile(get_inputset(), n_bits=bits)\n",
    "\n",
    "    return lora_trainer, train_dl\n",
    "\n",
    "\n",
    "def extract_lora_weights(model):\n",
    "    \"\"\"Extract LoRA weights from model for comparison.\"\"\"\n",
    "    weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and param.requires_grad:\n",
    "            weights[name] = param.detach().cpu().numpy().copy()\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Select only the first 5 batches for initial comparison\n",
    "def get_limited_batches(dataloader, num_batches=5):\n",
    "    \"\"\"Get a limited number of batches from dataloader.\"\"\"\n",
    "    limited_batches = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            limited_batches.append(batch)\n",
    "        else:\n",
    "            break\n",
    "    return limited_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8c6da",
   "metadata": {},
   "source": [
    "## Experiment: Comparing fhe=\"disable\" vs fhe=\"execute\"\n",
    "\n",
    "Train two identical models for 5 steps using the quantized clear model (disable mode) and compare against FHE fine-tuning (execute mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model trainer for reference\n",
    "lora_trainer = LoraTrainer(\n",
    "    model=peft_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=causal_lm_loss,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    training_args=vars(training_args),\n",
    "    n_layers_to_skip_for_backprop=3,\n",
    "    eval_loader=eval_dl,\n",
    "    eval_metric_fn=None,\n",
    "    logging_steps=1,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    train_log_path=TRAIN_LOG_FILE,\n",
    ")\n",
    "\n",
    "# Clear the existing file for our experiments\n",
    "with open(EVAL_RESPONSES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Experiment: disable vs simulate ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcf3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing first model (fhe=disable)...\")\n",
    "model_disable = initialize_model()\n",
    "initial_weights_disable = extract_lora_weights(model_disable)\n",
    "\n",
    "trainer_disable, train_dl = setup_trainer(model_disable)\n",
    "\n",
    "# Evaluate pre-training\n",
    "print(\"Evaluating pre-training model...\")\n",
    "model_disable.eval()\n",
    "pre_training_metrics = metric_fn(model_disable, eval_dl)\n",
    "print(f\"Pre-training perplexity: {pre_training_metrics['perplexity']:.2f}\")\n",
    "\n",
    "# Get limited batches for consistent training\n",
    "limited_batches = get_limited_batches(train_dl, 5)\n",
    "\n",
    "# Train with fhe=disable for 5 steps\n",
    "print(\"\\nTraining with fhe=disable for 5 steps...\")\n",
    "trainer_disable.train(limited_batches, fhe=\"disable\", device=DEVICE)\n",
    "losses_disable = trainer_disable.get_training_losses()\n",
    "\n",
    "# Store LoRA weights\n",
    "weights_disable = extract_lora_weights(model_disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInitializing second model (fhe=execute)...\")\n",
    "model_execute = initialize_model()\n",
    "trainer_execute, _ = setup_trainer(model_execute)\n",
    "\n",
    "# Verify weights are the same before training\n",
    "print(\"Verifying initial weights are identical...\")\n",
    "initial_weights_execute = extract_lora_weights(model_execute)\n",
    "weights_match = True\n",
    "for name in initial_weights_disable:\n",
    "    if not np.allclose(initial_weights_disable[name], initial_weights_execute[name]):\n",
    "        print(f\"Weight mismatch in {name}\")\n",
    "        weights_match = False\n",
    "if weights_match:\n",
    "    print(\"Initial weights match between models.\")\n",
    "\n",
    "# Train with fhe=execute for 5 steps\n",
    "print(\"\\nTraining with fhe=execute for 5 steps...\")\n",
    "trainer_execute.train(limited_batches, fhe=\"disable\", device=DEVICE)\n",
    "losses_execute = trainer_execute.get_training_losses()\n",
    "\n",
    "# Compare LoRA weights after training\n",
    "weights_execute = extract_lora_weights(model_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get x positions (steps)\n",
    "steps = np.arange(len(losses_disable))\n",
    "\n",
    "# Plot lines with enhanced visibility\n",
    "plt.plot(steps, losses_disable, label=\"Disable Loss\", color=\"blue\", linewidth=2.5, alpha=0.7)\n",
    "plt.plot(\n",
    "    steps,\n",
    "    losses_execute,\n",
    "    label=\"Execute Loss\",\n",
    "    color=\"red\",\n",
    "    linewidth=2.5,\n",
    "    alpha=0.7,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "# Add connecting lines to highlight differences\n",
    "for i in range(len(losses_disable)):\n",
    "    plt.plot(\n",
    "        [i, i], [losses_disable[i], losses_execute[i]], color=\"purple\", alpha=0.5, linewidth=1.5\n",
    "    )\n",
    "\n",
    "# Add points with distinct markers and sizes\n",
    "plt.scatter(steps, losses_disable, color=\"blue\", s=100, zorder=5, label=\"_nolegend_\")\n",
    "plt.scatter(steps, losses_execute, color=\"red\", marker=\"s\", s=100, zorder=5, label=\"_nolegend_\")\n",
    "\n",
    "plt.xlabel(\"Step\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Comparison of Losses with Highlighted Differences\", fontsize=14)\n",
    "plt.legend(fontsize=12, loc=\"upper right\")\n",
    "plt.grid(True, linestyle=\":\", alpha=0.4)\n",
    "\n",
    "# Set x-ticks to be at each step\n",
    "plt.xticks(steps)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b91fdf",
   "metadata": {},
   "source": [
    "## Continue Training for Full Convergence\n",
    "\n",
    "After validating the models, we'll continue training on the remaining batches (500-5 = 495 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Continuing Training for Full Convergence ===\")\n",
    "# We'll use the model_disable for further training\n",
    "model_final = initialize_model()\n",
    "trainer_final, full_train_dl = setup_trainer(model_final)\n",
    "\n",
    "# Get remaining batches (skip the first 5 we already used)\n",
    "remaining_batches = []\n",
    "for i, batch in enumerate(full_train_dl):\n",
    "    if i >= 5 and i < 500:\n",
    "        remaining_batches.append(batch)\n",
    "    elif i >= 500:\n",
    "        break\n",
    "\n",
    "print(f\"Training on {len(remaining_batches)} additional batches...\")\n",
    "trainer_final.train(remaining_batches, fhe=\"disable\", device=DEVICE)\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nEvaluating final model after full training...\")\n",
    "model_final.eval()\n",
    "metrics_final = metric_fn(model_final, eval_dl)\n",
    "print(f\"Final perplexity after extended training: {metrics_final['perplexity']:.2f}\")\n",
    "print(\n",
    "    f\"Improvement from initial model: {pre_training_metrics['perplexity'] - metrics_final['perplexity']:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a111c",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the fine-tuned model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PATH.is_dir() and any(SAVE_PATH.iterdir()):\n",
    "    shutil.rmtree(SAVE_PATH)\n",
    "trainer_final.save_and_clear_private_info(SAVE_PATH)\n",
    "print(\"Model saved to:\", SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
