{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA with LoRA for Math Word Problems\n",
    "\n",
    "This notebook demonstrates how to fine-tune the LLaMA-3.2-1B model using LoRA (Low-Rank Adaptation) on the Orca Math Word Problems dataset. We support multiple modes (Torch, 8-bit, 16-bit) and include evaluation metrics like perplexity.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from utils_lora import generate_and_print\n",
    "\n",
    "from concrete.ml.torch.lora import LoraTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Define the training mode and other parameters here. Modify these values as needed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode: 'torch', '8bit', or '16bit'\n",
    "MODE = \"8bit\"\n",
    "\n",
    "# Number of steps between evaluations\n",
    "EVAL_STEPS = 100\n",
    "\n",
    "# Set to True to force CPU\n",
    "FORCE_CPU = False\n",
    "\n",
    "# File paths based on mode\n",
    "mode_str = MODE\n",
    "EVAL_RESPONSES_FILE = f\"eval_generated_responses_{mode_str}.txt\"\n",
    "TRAIN_LOG_FILE = f\"training_log_{mode_str}.txt\"\n",
    "SAVE_PATH = Path(f\"deployment/llama_lora_finetuned_{mode_str}\")\n",
    "\n",
    "print(f\"Mode: {MODE}, Eval Steps: {EVAL_STEPS}, Force CPU: {FORCE_CPU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and Seed Configuration\n",
    "\n",
    "Set up the device (CPU/GPU/MPS) and random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if FORCE_CPU:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "DEVICE = get_device()\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the LLaMA model and tokenizer, and test the base model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "PROMPT = \"When you multiply a number by 7, it becomes 98. What is that number?\\n\"\n",
    "_ = generate_and_print(PROMPT, model, tokenizer, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Set up LoRA parameters and apply them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Load the dataset, filter by length, and preprocess for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "raw_dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train\")\n",
    "\n",
    "\n",
    "def length_filter(example):\n",
    "    q_len = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    a_len = len(tokenizer(example[\"answer\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    return (q_len + a_len + 1) <= MAX_LENGTH\n",
    "\n",
    "\n",
    "filtered_dataset = raw_dataset.filter(length_filter)\n",
    "\n",
    "\n",
    "def get_lengths(example):\n",
    "    q_len = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    a_len = len(tokenizer(example[\"answer\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    total_len = q_len + a_len + 1\n",
    "    return {\"q_len\": q_len, \"a_len\": a_len, \"total_len\": total_len}\n",
    "\n",
    "\n",
    "lengths = filtered_dataset.map(get_lengths)\n",
    "q_lengths = [x[\"q_len\"] for x in lengths]\n",
    "a_lengths = [x[\"a_len\"] for x in lengths]\n",
    "total_lengths = [x[\"total_len\"] for x in lengths]\n",
    "\n",
    "print(\"\\nLength Distribution Statistics:\")\n",
    "print(f\"Original dataset size: {len(raw_dataset):,}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset):,}\")\n",
    "print(f\"Percentage kept: {100 * len(filtered_dataset)/len(raw_dataset):.1f}%\\n\")\n",
    "print(\"Question lengths: \")\n",
    "print(f\"  Min: {min(q_lengths)}, Max: {max(q_lengths)}\")\n",
    "print(f\"  Mean: {sum(q_lengths)/len(q_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(q_lengths)[len(q_lengths)//2]}\")\n",
    "print(\"\\nAnswer lengths:\")\n",
    "print(f\"  Min: {min(a_lengths)}, Max: {max(a_lengths)}\")\n",
    "print(f\"  Mean: {sum(a_lengths)/len(a_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(a_lengths)[len(a_lengths)//2]}\")\n",
    "print(\"\\nTotal lengths (including newline):\")\n",
    "print(f\"  Min: {min(total_lengths)}, Max: {max(total_lengths)}\")\n",
    "print(f\"  Mean: {sum(total_lengths)/len(total_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(total_lengths)[len(total_lengths)//2]}\\n\")\n",
    "\n",
    "\n",
    "def process_example(example):\n",
    "    question = example[\"question\"].strip()\n",
    "    answer = example[\"answer\"].strip()\n",
    "    tokens = tokenizer(\n",
    "        question + \"\\n\" + answer,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    question_length = len(tokenizer(question, add_special_tokens=False)[\"input_ids\"]) + 1\n",
    "    labels = tokens[\"input_ids\"].copy()\n",
    "    for i in range(question_length):\n",
    "        if i < len(labels):\n",
    "            labels[i] = -100\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_dataset = filtered_dataset.map(\n",
    "    process_example,\n",
    "    batched=False,\n",
    "    remove_columns=filtered_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized = tokenized_dataset.train_test_split(test_size=0.05, seed=SEED, shuffle=True)\n",
    "train_dataset, test_dataset = tokenized[\"train\"], tokenized[\"test\"]\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "Define a custom data collator to handle padding while preserving label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        inputs, attention_masks, labels = [], [], []\n",
    "        max_length = max(len(example[\"input_ids\"]) for example in examples)\n",
    "        for example in examples:\n",
    "            inputs.append(example[\"input_ids\"])\n",
    "            attention_masks.append(example[\"attention_mask\"])\n",
    "            labels.append(example[\"labels\"])\n",
    "\n",
    "        def pad(sequences, value):\n",
    "            return [x + [value] * (max_length - len(x)) for x in sequences]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(pad(inputs, self.pad_id)),\n",
    "            \"attention_mask\": torch.tensor(pad(attention_masks, 0)),\n",
    "            \"labels\": torch.tensor(pad(labels, -100)),\n",
    "        }\n",
    "\n",
    "\n",
    "collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments\n",
    "\n",
    "Configure the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Evaluation Metrics\n",
    "\n",
    "Define the loss function and evaluation metric (perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_lm_loss(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    return F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=-100,\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "\n",
    "\n",
    "def metric_fn(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens, results = 0.0, 0, []\n",
    "    response = generate_and_print(PROMPT, model, tokenizer, seed=SEED)\n",
    "    if response:\n",
    "        results.append({\"prompt\": PROMPT, \"response\": response})\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            batch_labels = batch[\"labels\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "            valid = batch_labels[..., 1:] != -100\n",
    "            loss = F.cross_entropy(\n",
    "                outputs[..., :-1, :].contiguous().view(-1, outputs.size(-1)),\n",
    "                batch_labels[..., 1:].contiguous().view(-1),\n",
    "                ignore_index=-100,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += valid.sum().item()\n",
    "    perplexity = math.exp(total_loss / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "    with open(EVAL_RESPONSES_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Perplexity: {perplexity:.2f}\\n\")\n",
    "        for i, r in enumerate(results):\n",
    "            f.write(\n",
    "                f\"== Generation {i+1} ==\\nPrompt:\\n{r['prompt']}\\n\\nResponse:\\n{r['response']}\\n\"\n",
    "            )\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Setup\n",
    "\n",
    "Initialize the Hugging Face Trainer to set up the optimizer and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "train_dl = hf_trainer.get_train_dataloader()\n",
    "hf_trainer.create_optimizer_and_scheduler(len(train_dl) * training_args.num_train_epochs)\n",
    "optimizer, lr_scheduler = hf_trainer.optimizer, hf_trainer.lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration and Evaluation Data\n",
    "\n",
    "Prepare dummy calibration data and the evaluation DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputset = {\n",
    "    \"input_ids\": torch.randint(0, tokenizer.vocab_size, (4, MAX_LENGTH)),\n",
    "    \"attention_mask\": torch.ones((4, MAX_LENGTH), dtype=torch.long),\n",
    "    \"labels\": torch.randint(0, tokenizer.vocab_size, (4, MAX_LENGTH)),\n",
    "}\n",
    "\n",
    "eval_dl = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Trainer Initialization\n",
    "\n",
    "Set up the LoRA trainer with the model, optimizer, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainer = LoraTrainer(\n",
    "    model=peft_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=causal_lm_loss,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    training_args=vars(training_args),\n",
    "    n_layers_to_skip_for_backprop=3,\n",
    "    eval_loader=eval_dl,\n",
    "    eval_metric_fn=metric_fn,\n",
    "    logging_steps=1,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    train_log_path=TRAIN_LOG_FILE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation\n",
    "\n",
    "Compile the model for 8-bit or 16-bit precision if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"torch\":\n",
    "    print(\"Using Torch mode (no compilation).\")\n",
    "else:\n",
    "    bits = 8 if MODE == \"8bit\" else 16\n",
    "    print(f\"Compiling model with {bits} bits...\")\n",
    "    lora_trainer.compile(inputset, n_bits=bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Train the model and evaluate it before and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EVAL_RESPONSES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Training Start ===\\n\")\n",
    "fhe_mode = \"torch\" if MODE == \"torch\" else \"disable\"\n",
    "peft_model.to(DEVICE)\n",
    "\n",
    "# Pre-training evaluation\n",
    "print(\"Evaluating quantized model before training...\")\n",
    "peft_model.disable_adapter_layers()\n",
    "original_metrics = metric_fn(peft_model, eval_dl)\n",
    "print(\"Original model metrics:\", original_metrics)\n",
    "peft_model.enable_adapter_layers()\n",
    "\n",
    "# Training\n",
    "print(f\"\\nTraining in {MODE} mode...\")\n",
    "lora_trainer.train(train_dl, fhe=fhe_mode, device=DEVICE)\n",
    "print(\"Training losses:\", lora_trainer.get_training_losses())\n",
    "\n",
    "# Post-training evaluation\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "final_metrics = metric_fn(peft_model, eval_dl)\n",
    "print(\"Fine-tuned model metrics:\", final_metrics)\n",
    "\n",
    "print(\"\\nMetrics comparison:\")\n",
    "print(f\"Original perplexity: {original_metrics['perplexity']:.2f}\")\n",
    "print(f\"Fine-tuned perplexity: {final_metrics['perplexity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Generations\n",
    "\n",
    "Compare outputs from the original and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.disable_adapter_layers()\n",
    "print(\"Original model says:\", generate_and_print(PROMPT, peft_model, tokenizer, seed=SEED))\n",
    "peft_model.enable_adapter_layers()\n",
    "print(\"Fine-tuned model says:\", generate_and_print(PROMPT, peft_model, tokenizer, seed=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the fine-tuned model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PATH.is_dir() and any(SAVE_PATH.iterdir()):\n",
    "    shutil.rmtree(SAVE_PATH)\n",
    "lora_trainer.save_and_clear_private_info(SAVE_PATH)\n",
    "print(\"Model saved to:\", SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
