**1. Linear Models:**
*   **Logistic Regression:**
python
from concrete.ml.sklearn import LogisticRegression as ConcreteLogisticRegression
# ... (Data loading and preprocessing) ...
concrete_logr = ConcreteLogisticRegression(n_bits=8)
concrete_logr.fit(x_train, y_train)
fhe_circuit = concrete_logr.compile(x_train)
# Key generation
fhe_circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = concrete_logr.predict(x_test, fhe="execute")

*   **Linear Regression:**
python
from concrete.ml.sklearn import LinearRegression as ConcreteLinearRegression
# ... (Data loading and preprocessing) ...
concrete_lr = ConcreteLinearRegression(n_bits=8)
concrete_lr.fit(x_train, y_train)
fhe_circuit = concrete_lr.compile(x_train)
# Key generation
fhe_circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = concrete_lr.predict(x_test, fhe="execute")

*   **Linear SVR:**
python
from concrete.ml.sklearn.svm import LinearSVR as ConcreteLinearSVR
# ... (Data loading and preprocessing) ...
concrete_svr = ConcreteLinearSVR(n_bits=8, C=0.5)
concrete_svr.fit(x_train, y_train)
circuit = concrete_svr.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = concrete_svr.predict(x_test, fhe="execute")

*   **Linear SVC**
python
from concrete.ml.sklearn.svm import LinearSVC as ConcreteLinearSVC
# ... (Data loading and preprocessing) ...
concrete_svc = ConcreteLinearSVC(n_bits=8, C=0.025)
concrete_svc.fit(x_train, y_train)
circuit = concrete_svc.compile(x_train)
# Inference in FHE
y_pred_fhe = concrete_svc.predict(x_test, fhe="execute")

**2. Tree-Based Models:**
*   **XGBoost Classifier:**
python
from concrete.ml.sklearn import XGBClassifier as ConcreteXGBClassifier
# ... (Data loading and preprocessing) ...
concrete_xgb = ConcreteXGBClassifier(n_bits=6, n_estimators=50, max_depth=4)
concrete_xgb.fit(x_train, y_train)
circuit = concrete_xgb.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_preds_fhe = concrete_xgb.predict(x_test, fhe="execute")

*   **XGBoost Regressor:**
python
from concrete.ml.sklearn import XGBRegressor as ConcreteXGBRegressor
# ... (Data loading and preprocessing) ...
concrete_xgb = ConcreteXGBRegressor(n_bits=6, n_estimators=50, max_depth=4)
concrete_xgb.fit(x_train, y_train)
circuit = concrete_xgb.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_preds_fhe = concrete_xgb.predict(x_test, fhe="execute")

*   **Decision Tree Classifier:**
python
from concrete.ml.sklearn import DecisionTreeClassifier as ConcreteDecisionTreeClassifier
# ... (Data loading and preprocessing) ...
model = ConcreteDecisionTreeClassifier(
    max_features="log2",
    min_samples_leaf=1,
    min_samples_split=2,
    max_depth=6,
    n_bits=6,
)
model.fit(x_train, y_train)
circuit = model.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = model.predict(x_test, fhe="execute")

*   **Decision Tree Regressor:**
python
from concrete.ml.sklearn import DecisionTreeRegressor as ConcreteDecisionTreeRegressor
# ... (Data loading and preprocessing) ...
model = ConcreteDecisionTreeRegressor(
    max_depth=10,
    max_features=5,
    min_samples_leaf=2,
    min_samples_split=10,
    n_bits=6,
    random_state=42,
)
model.fit(x_train, y_train)
circuit = model.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = model.predict(x_test, fhe="execute")

*   **Random Forest Classifier:**
python
from concrete.ml.sklearn import RandomForestClassifier
# ... (Data loading and preprocessing) ...
model = RandomForestClassifier(max_depth=4, n_estimators=5, n_bits=5)
model.fit(x_train, y_train)
circuit = model.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = model.predict(x_test, fhe="execute")

*   **Random Forest Regressor:**
python
from concrete.ml.sklearn import RandomForestRegressor
# ... (Data loading and preprocessing) ...
model = RandomForestRegressor(n_bits=5, n_estimators=50, max_depth=4)
model.fit(x_train, y_train)
circuit = model.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = model.predict(x_test, fhe="execute")

**3. Neural Networks:**
*   **Fully Connected Neural Network:**
python
from torch import nn
from concrete.ml.sklearn import NeuralNetClassifier
# ... (Data loading and preprocessing) ...
parameters_neural_net = {
    "module__n_w_bits": 2,
    "module__n_a_bits": 4,
    "module__n_accum_bits": 32,
    "module__n_hidden_neurons_multiplier": 6,
    "module__n_layers": 2,  # 1 hidden layer
    "module__activation_function": nn.ReLU,
    "max_epochs": 400,
    "verbose": 0,
    "lr": 0.001,
}
model = NeuralNetClassifier(batch_size=32, **parameters_neural_net)
model.fit(X=x_train, y=y_train)
fhe_circuit = model.compile(x_train)
# Key generation
fhe_circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = model.predict(x_test, fhe="execute")

*   **Convolutional Neural Network:**
python
import torch
from torch import nn
from concrete.ml.torch.compile import compile_torch_model
# ... (Data loading and preprocessing) ...
class TinyCNN(nn.Module):
    def __init__(self, n_classes) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 8, 3, stride=1, padding=0)
        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=0)
        self.conv3 = nn.Conv2d(16, 32, 2, stride=1, padding=0)
        self.fc1 = nn.Linear(32, n_classes)
    def forward(self, x):
        x = self.conv1(x)
        x = torch.relu(x)
        x = self.conv2(x)
        x = torch.relu(x)
        x = self.conv3(x)
        x = torch.relu(x)
        x = x.flatten(1)
        x = self.fc1(x)
        return x
net = TinyCNN(10)
# ... (Training loop) ...
q_module = compile_torch_model(net, x_train, rounding_threshold_bits=6, p_error=0.1)
# Key generation
q_module.fhe_circuit.keygen()
# Inference in FHE
y_pred_fhe = q_module.forward(x_test, fhe="execute")

**4. Quantization-Aware Training:**
python
from torch import nn
from concrete.ml.torch.compile import compile_brevitas_qat_model
import brevitas.nn as qnn
from brevitas.core.bit_width import BitWidthImplType
from brevitas.core.quant import QuantType
from brevitas.core.restrict_val import FloatToIntImplType, RestrictValueType
from brevitas.core.scaling import ScalingImplType
from brevitas.core.zero_point import ZeroZeroPoint
from brevitas.inject import ExtendedInjector
from brevitas.quant.solver import ActQuantSolver, WeightQuantSolver
from dependencies import value
from torch.nn.utils import prune
# ... (Data loading and preprocessing) ...
class CommonQuant(ExtendedInjector):
    bit_width_impl_type = BitWidthImplType.CONST
    scaling_impl_type = ScalingImplType.CONST
    restrict_scaling_type = RestrictValueType.FP
    zero_point_impl = ZeroZeroPoint
    float_to_int_impl_type = FloatToIntImplType.ROUND
    scaling_per_output_channel = False
    narrow_range = True
    signed = True
    @value
    def quant_type(bit_width):  # pylint: disable=no-self-argument
        if bit_width is None:
            return QuantType.FP
        if bit_width == 1:
            return QuantType.BINARY
        return QuantType.INT
class CommonWeightQuant(CommonQuant, WeightQuantSolver):  # pylint: disable=too-many-ancestors
    scaling_const = 1.0
    signed = True
class CommonActQuant(CommonQuant, ActQuantSolver):  # pylint: disable=too-many-ancestors
    min_val = -1.0
    max_val = 1.0
class QATPrunedSimpleNet(nn.Module):
    def __init__(self, n_hidden, qlinear_args, qidentity_args):
        super().__init__()
        self.pruned_layers = set()
        self.quant_inp = qnn.QuantIdentity(**qidentity_args)
        self.fc1 = qnn.QuantLinear(IN_FEAT, n_hidden, **qlinear_args)
        self.relu1 = qnn.QuantReLU(bit_width=qidentity_args["bit_width"])
        self.fc2 = qnn.QuantLinear(n_hidden, n_hidden, **qlinear_args)
        self.relu2 = qnn.QuantReLU(bit_width=qidentity_args["bit_width"])
        self.fc3 = qnn.QuantLinear(n_hidden, OUT_FEAT, **qlinear_args)
        for m in self.modules():
            if isinstance(m, qnn.QuantLinear):
                torch.nn.init.uniform_(m.weight.data, -1, 1)
    def forward(self, x):
        x = self.quant_inp(x)
        x = self.relu1(self.fc1(x))
        x = self.relu2(self.fc2(x))
        x = self.fc3(x)
        return x
    def prune(self, max_non_zero):
        # Linear layer weight has dimensions NumOutputs x NumInputs
        for name, layer in self.named_modules():
            if isinstance(layer, qnn.QuantLinear):
                num_zero_weights = (layer.weight.shape[1] - max_non_zero) * layer.weight.shape[0]
                if num_zero_weights <= 0:
                    continue
                print(f"Pruning layer {name} factor {num_zero_weights}")
                prune.l1_unstructured(layer, "weight", amount=num_zero_weights)
                self.pruned_layers.add(name)
    def unprune(self):
        for name, layer in self.named_modules():
            if name in self.pruned_layers:
                prune.remove(layer, "weight")
                self.pruned_layers.remove(name)
torch_model = QATPrunedSimpleNet(
    n_hidden=n_hidden,
    qlinear_args={
        "weight_bit_width": 3,
        "weight_quant": CommonWeightQuant,
        "bias": True,
        "bias_quant": None,
        "narrow_range": True,
    },
    qidentity_args={"bit_width": 3, "act_quant": CommonActQuant},
)
torch_model.prune(20)
# ... (Training loop) ...
quantized_numpy_module = compile_brevitas_qat_model(torch_model, x_train)
# Inference in FHE (simulation)
y_pred_fhe = quantized_numpy_module.forward(x_test, fhe="simulate")

**5. Client/Server Deployment (LogisticRegressionTraining.ipynb):**
python
from pathlib import Path
from tempfile import TemporaryDirectory
import numpy as np
from concrete.ml.deployment import FHEModelClient, FHEModelDev, FHEModelServer
from concrete.ml.sklearn import SGDClassifier
from concrete import fhe
# ... (Data loading, preprocessing, and model training) ...
# Assuming you have a trained model: sgd_clf_binary_fhe
# and x_compile_set, y_compile_set for compilation
# Define the directory where to save the deployment files
DEPLOYMENT_PATH = Path("fhe_training")
DEPLOYMENT_PATH.mkdir(exist_ok=True)
deployment_dir = TemporaryDirectory(dir=str(DEPLOYMENT_PATH))
deployment_path = Path(deployment_dir.name)
# Save the model for deployment
fhe_dev = FHEModelDev(deployment_path, sgd_clf_binary_fhe)
fhe_dev.save(mode="training")
# Client-side setup
fhe_client = FHEModelClient(deployment_path)
fhe_client.load()
serialized_evaluation_keys = fhe_client.get_serialized_evaluation_keys()
# Server-side setup
fhe_server = FHEModelServer(deployment_path)
fhe_server.load()
# Example of encryption, server-side processing, and decryption
batch_size = sgd_clf_binary_fhe.batch_size
weights = np.random.rand(1, x_train.shape[1], 1)
bias = np.random.rand(1, 1, 1)
def quantize_encrypt_serialize_batches(fhe_client, x, y, weights, bias, batch_size):
    # ... (Implementation as before) ...
def server_run(fhe_server, x_batches_enc, y_batches_enc, weights_enc, bias_enc, evaluation_keys):
    # ... (Implementation as before) ...
def train_fhe_client_server(
    # ... (Parameters as before) ...
):
    # ... (Training loop)
    # Quantize, encrypt and serialize the batched inputs as well as the weight and bias values
    x_batches_enc, y_batches_enc, weights_enc, bias_enc = quantize_encrypt_serialize_batches(
        fhe_client, x, y, weights, bias, batch_size
    )
    # Iterate the circuit over the batches on the server
    fitted_weights_enc, fitted_bias_enc = server_run(
        fhe_server,
        x_batches_enc,
        y_batches_enc,
        weights_enc,
        bias_enc,
        serialized_evaluation_keys,
    )
    # Back on the client, deserialize, decrypt and de-quantize the fitted weight and bias values
    weights, bias = fhe_client.deserialize_decrypt_dequantize(
        fitted_weights_enc, fitted_bias_enc
    )
    return weights, bias, acc_history
# Cleanup
deployment_dir.cleanup()

**6. Hyper-parameter Tuning with GridSearchCV (XGBClassifier.ipynb, DecisionTreeRegressor.ipynb):**
python
from sklearn.model_selection import GridSearchCV
from concrete.ml.sklearn import XGBClassifier as ConcreteXGBClassifier
from sklearn.metrics import make_scorer, matthews_corrcoef
# ... (Data loading and preprocessing) ...
# Create scorer with the MCC metric
grid_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)
# Define the parameter grid to search
param_grid = {
    "n_bits": [5, 6],
    "max_depth": [2, 3],
    "n_estimators": [10, 20, 50],
}
# Instantiate GridSearchCV with the Concrete ML model
grid_search = GridSearchCV(
    ConcreteXGBClassifier(),
    param_grid,
    cv=5,
    scoring=grid_scorer,
    error_score="raise",
    verbose=1,
)
# Run the grid search
grid_search.fit(x_train, y_train)
# Get the best parameters
best_params = grid_search.best_params_
# Create a new model with the best parameters
best_model = ConcreteXGBClassifier(**best_params)
best_model.fit(x_train, y_train)
# Compile and proceed with FHE inference as shown in other examples

**7. GLM Models (GLMComparison.ipynb):**
*   **Poisson Regressor**
python
from concrete.ml.sklearn import PoissonRegressor as ConcretePoissonRegressor
# ... (Data loading and preprocessing) ...
concrete_pr = ConcretePoissonRegressor(n_bits=8)
concrete_pr.fit(x_train, y_train, sample_weight=train_weights)
circuit = concrete_pr.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = concrete_pr.predict(x_test, fhe="execute")

*   **Gamma Regressor**
python
from concrete.ml.sklearn import GammaRegressor as ConcreteGammaRegressor
# ... (Data loading and preprocessing) ...
concrete_gr = ConcreteGammaRegressor(n_bits=8)
concrete_gr.fit(x_train, y_train, sample_weight=train_weights)
circuit = concrete_gr.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = concrete_gr.predict(x_test, fhe="execute")

*   **Tweedie Regressor**
python
from concrete.ml.sklearn import TweedieRegressor as ConcreteTweedieRegressor
# ... (Data loading and preprocessing) ...
concrete_tr = ConcreteTweedieRegressor(n_bits=8, power=1.9)
concrete_tr.fit(x_train, y_train, sample_weight=train_weights)
circuit = concrete_tr.compile(x_train)
# Key generation
circuit.client.keygen(force=False)
# Inference in FHE
y_pred_fhe = concrete_tr.predict(x_test, fhe="execute")

**8. Fine-tuning with LoRA (LoraMLP.ipynb):**
python
import torch
from peft import LoraConfig, get_peft_model
from torch import nn, optim
from concrete.ml.torch.lora import LoraTrainer
# ... (Data loading and preprocessing) ...
# Define an MLP model without LoRA layers
class SimpleMLP(nn.Module):
    def __init__(self, input_size=2, hidden_size=128, num_classes=2):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out
# Instantiate the model
model = SimpleMLP()
# ... (Training loop for Task 1) ...
# Apply LoRA to the model using peft
lora_config = LoraConfig(
    r=1, lora_alpha=1, lora_dropout=0.01, target_modules=["fc1", "fc2"], bias="none"
)
peft_model = get_peft_model(model, lora_config)
# Update training parameters, including loss function
optimizer = optim.Adam(filter(lambda p: p.requires_grad, peft_model.parameters()), lr=0.01)
loss_fn = nn.CrossEntropyLoss()
training_args = {"gradient_accumulation_steps": 1}
# Set up LoRA training
lora_trainer = LoraTrainer(peft_model, optimizer=optimizer, loss_fn=loss_fn, training_args=training_args)
# Prepare input data for calibration
batch_size_per_task = batch_size // 2
inputset = (
    torch.cat([X_task1[:batch_size_per_task], X_task2[:batch_size_per_task]]),
    torch.cat([y_task1[:batch_size_per_task], y_task2[:batch_size_per_task]]),
)
# Compile the model
lora_trainer.compile(inputset, n_bits=8)
# Fine-tune the model on Task 2 using LoRA
lora_trainer.train(train_loader_task2, num_epochs=10, fhe="execute")
# Enable/Disable LoRA adapters
peft_model.enable_adapter_layers()
peft_model.disable_adapter_layers()
# Print trainable (lora) parameters
peft_model.print_trainable_parameters()
# Save the model and remove all layers that will be done on the server
path = Path("lora_mlp")
if path.is_dir() and any(path.iterdir()):
    shutil.rmtree(path)
lora_trainer.save_and_clear_private_info(path)
