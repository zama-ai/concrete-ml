{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from custom_module import CustomConv1D\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Conv1D,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# FREEZE WEIGHTS\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=30, fhe=\"disable\"):\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FH: A basic program that is used to calculate the height of an object, and then sets the minimum height to be\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_conv1d(module, module_index_to_skip=0):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, Conv1D):\n",
    "\n",
    "            # Skip the module if the index has not been reached, and decrement the index\n",
    "            if module_index_to_skip >= 0:\n",
    "                module_index_to_skip -= 1\n",
    "            else:\n",
    "                custom_linear = CustomConv1D(child.weight, bias=child.bias)\n",
    "                setattr(module, name, custom_linear)\n",
    "        else:\n",
    "            module_index_to_skip = replace_conv1d(child, module_index_to_skip=module_index_to_skip)\n",
    "\n",
    "    return module_index_to_skip\n",
    "\n",
    "\n",
    "# Gradients of the first base layer that is used for fine-tuning are not needed. We\n",
    "# therefore need to exclude the backward module from the remote_names since calibration\n",
    "# won't get through it (which raises an issue with hybrid models)\n",
    "replace_conv1d(peft_model, module_index_to_skip=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraTraining(torch.nn.Module):\n",
    "    def __init__(self, inference_model, gradient_accumulation_steps) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.inference_model = inference_model\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.lr_scheduler = None\n",
    "\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.max_grad_norm = None\n",
    "\n",
    "        self.calibrate = False\n",
    "        self.run_optimizer = False\n",
    "\n",
    "    def update_training_parameters(self, optimizer, lr_scheduler, training_args):\n",
    "        assert self.gradient_accumulation_steps == training_args.gradient_accumulation_steps\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.max_grad_norm = training_args.max_grad_norm\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # FIXME: handle multi-inputs in hybrid model\n",
    "        x, y = inputs\n",
    "\n",
    "        # some parts on server side\n",
    "        outputs = self.inference_model(input_ids=x, labels=y)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "        # Update gradients\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = None\n",
    "        if not self.calibrate and self.run_optimizer:\n",
    "            assert self.optimizer is not None\n",
    "            assert self.lr_scheduler is not None\n",
    "            assert self.max_grad_norm is not None\n",
    "\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                self.inference_model.parameters(), max_norm=self.max_grad_norm, norm_type=2\n",
    "            )\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "            self.inference_model.zero_grad()\n",
    "\n",
    "        # Clean gradients after calibration\n",
    "        elif self.calibrate:\n",
    "            self.inference_model.zero_grad()\n",
    "\n",
    "        return (loss, grad_norm)\n",
    "\n",
    "    def toggle_calibrate(self, enable: bool = True):\n",
    "        self.calibrate = enable\n",
    "\n",
    "    def toggle_run_optimizer(self, enable: bool = True):\n",
    "        self.run_optimizer = enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "lora_training = LoraTraining(peft_model, GRADIENT_ACCUMULATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 128\n",
    "\n",
    "\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=BLOCK_SIZE,\n",
    "        cache_dir=\"cache_dataset\",\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"data_finetune/what_is_fhe.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.parallelism = False\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "EPOCHS = 2\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=5e-4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=0,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = len_dataloader // training_args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "max_steps = math.ceil(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
    "\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_training.update_training_parameters(trainer.optimizer, trainer.lr_scheduler, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remote_names(model):\n",
    "    remote_names = []\n",
    "    for name, module in model.named_modules():\n",
    "        # Some gradients are not needed for fine-tuning, so need to exclude the backward module\n",
    "        # from the remote_names since calibration won't get through it (which raises an issue with\n",
    "        # hybrid models). We however still need to include the associated module's forward pass in\n",
    "        # the hybrid model\n",
    "        if isinstance(module, Conv1D):\n",
    "            remote_names.append(name)\n",
    "\n",
    "        elif isinstance(module, CustomConv1D):\n",
    "            remote_names.append(name + \".forward_module\")\n",
    "            remote_names.append(name + \".backward_module\")\n",
    "\n",
    "    return remote_names\n",
    "\n",
    "\n",
    "remote_names = get_remote_names(lora_training)\n",
    "\n",
    "hybrid_model = HybridFHEModel(lora_training, module_names=remote_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "label_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "\n",
    "inputset = (input_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.model.toggle_calibrate(enable=True)\n",
    "\n",
    "hybrid_model.compile_model(\n",
    "    inputset, n_bits=8, rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"}\n",
    ")\n",
    "\n",
    "hybrid_model.model.toggle_calibrate(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"disable\"):\n",
    "    device = \"cpu\"\n",
    "    hybrid_model.model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    hybrid_model.model.inference_model.train()\n",
    "\n",
    "    total_epochs = int(training_args.num_train_epochs)\n",
    "    epoch_pbar = tqdm(total=total_epochs, desc=\"Training Progress\", position=0)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        grad_norms = []\n",
    "\n",
    "        steps_in_epoch = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Gradient accumulation\n",
    "            is_last_batch_step = (\n",
    "                steps_in_epoch <= training_args.gradient_accumulation_steps\n",
    "                and (step + 1) == steps_in_epoch\n",
    "            )\n",
    "            accumulate_gradients = (\n",
    "                total_batched_samples % training_args.gradient_accumulation_steps == 0\n",
    "            )\n",
    "\n",
    "            run_optimizer = is_last_batch_step or accumulate_gradients\n",
    "\n",
    "            hybrid_model.model.toggle_run_optimizer(enable=run_optimizer)\n",
    "\n",
    "            loss, grad_norm = hybrid_model((batch[\"input_ids\"], batch[\"labels\"]), fhe=fhe)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if grad_norm is not None:\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = hybrid_model.model.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Get last grad norm\n",
    "        current_grad_norm = grad_norms[-1]\n",
    "\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, \"\n",
    "            f\"Loss: {total_loss:.4f}, grad norm: {current_grad_norm}, lr: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if training_args.output_dir is not None:\n",
    "        save_path = f\"{training_args.output_dir}/checkpoint-{epoch + 1}\"\n",
    "        hybrid_model.model.inference_model.save_pretrained(save_path)\n",
    "\n",
    "    epoch_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Progress:  50%|█████     | 1/2 [02:53<02:53, 173.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 1.8678, grad norm: 0.20942462980747223, lr: 0.00025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(SEED)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhybrid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfhe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 35\u001b[0m, in \u001b[0;36mtrain_custom_model\u001b[0;34m(hybrid_model, train_dataloader, training_args, fhe)\u001b[0m\n\u001b[1;32m     31\u001b[0m run_optimizer \u001b[38;5;241m=\u001b[39m is_last_batch_step \u001b[38;5;129;01mor\u001b[39;00m accumulate_gradients\n\u001b[1;32m     33\u001b[0m hybrid_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtoggle_run_optimizer(enable\u001b[38;5;241m=\u001b[39mrun_optimizer)\n\u001b[0;32m---> 35\u001b[0m loss, grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfhe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfhe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/ml/torch/hybrid_model.py:419\u001b[0m, in \u001b[0;36mHybridFHEModel.__call__\u001b[0;34m(self, x, fhe)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    418\u001b[0m     module\u001b[38;5;241m.\u001b[39mfhe_local_mode \u001b[38;5;241m=\u001b[39m HybridFHEMode(fhe)\n\u001b[0;32m--> 419\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[25], line 34\u001b[0m, in \u001b[0;36mLoraTraining.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Update gradients\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalibrate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_optimizer:\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/torch/autograd/function.py:267\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/concrete-ml/use_case_examples/lora_finetune/custom_module.py:36\u001b[0m, in \u001b[0;36mForwardBackwardModule.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[1;32m     35\u001b[0m     backward_module \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mbackward_module\n\u001b[0;32m---> 36\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# grad_weight and grad_bias are not needed when computing the backward for lora\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_input, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/ml/torch/hybrid_model.py:253\u001b[0m, in \u001b[0;36mRemoteModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhe_local_mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    245\u001b[0m     HybridFHEMode\u001b[38;5;241m.\u001b[39mDISABLE,\n\u001b[1;32m    246\u001b[0m     HybridFHEMode\u001b[38;5;241m.\u001b[39mCALIBRATE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m }:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Using quantized module\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivate_q_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\n\u001b[0;32m--> 253\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprivate_q_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfhe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfhe_local_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhe_local_mode \u001b[38;5;241m==\u001b[39m HybridFHEMode\u001b[38;5;241m.\u001b[39mDISABLE:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Calling torch\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivate_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/ml/quantization/quantized_module.py:443\u001b[0m, in \u001b[0;36mQuantizedModule.forward\u001b[0;34m(self, fhe, debug, *x)\u001b[0m\n\u001b[1;32m    440\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequantize_output(\u001b[38;5;241m*\u001b[39mto_tuple(q_y_pred))\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, debug_value_tracker\n\u001b[0;32m--> 443\u001b[0m q_y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfhe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfhe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# De-quantize the output predicted values\u001b[39;00m\n\u001b[1;32m    446\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequantize_output(\u001b[38;5;241m*\u001b[39mto_tuple(q_y_pred))\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/ml/quantization/quantized_module.py:486\u001b[0m, in \u001b[0;36mQuantizedModule.quantized_forward\u001b[0;34m(self, fhe, *q_x)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_forward(\u001b[38;5;241m*\u001b[39mq_x)\n\u001b[1;32m    485\u001b[0m simulate \u001b[38;5;241m=\u001b[39m fhe \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fhe_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/ml/quantization/quantized_module.py:651\u001b[0m, in \u001b[0;36mQuantizedModule._fhe_forward\u001b[0;34m(self, simulate, *q_x)\u001b[0m\n\u001b[1;32m    648\u001b[0m     predict_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhe_circuit\u001b[38;5;241m.\u001b[39mencrypt_run_decrypt\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Execute the forward pass in FHE or with simulation\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m q_result \u001b[38;5;241m=\u001b[39m to_tuple(\u001b[43mpredict_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq_input\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(q_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(q_result_by_output), (\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of outputs does not match the number of output quantizers.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(q_result)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quantizers)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elt_index, elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(q_result):\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/fhe/compilation/circuit.py:168\u001b[0m, in \u001b[0;36mCircuit.simulate\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    165\u001b[0m ordered_validated_args \u001b[38;5;241m=\u001b[39m validate_input_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mclient_specs, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    167\u001b[0m exporter \u001b[38;5;241m=\u001b[39m SimulatedValueExporter\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mclient_specs\u001b[38;5;241m.\u001b[39mclient_parameters)\n\u001b[0;32m--> 168\u001b[0m exported \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    169\u001b[0m     (\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m Value(\n\u001b[1;32m    173\u001b[0m             exporter\u001b[38;5;241m.\u001b[39mexport_tensor(position, arg\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;28mlist\u001b[39m(arg\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m ()\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mexport_scalar(position, \u001b[38;5;28mint\u001b[39m(arg))\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m position, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ordered_validated_args)\n\u001b[1;32m    179\u001b[0m ]\n\u001b[1;32m    181\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39mexported)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/concrete-ml/.venv/lib/python3.10/site-packages/concrete/fhe/compilation/circuit.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m ordered_validated_args \u001b[38;5;241m=\u001b[39m validate_input_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mclient_specs, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    167\u001b[0m exporter \u001b[38;5;241m=\u001b[39m SimulatedValueExporter\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mclient_specs\u001b[38;5;241m.\u001b[39mclient_parameters)\n\u001b[1;32m    168\u001b[0m exported \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    169\u001b[0m     (\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m Value(\n\u001b[0;32m--> 173\u001b[0m             \u001b[43mexporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m ()\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mexport_scalar(position, \u001b[38;5;28mint\u001b[39m(arg))\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m position, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ordered_validated_args)\n\u001b[1;32m    179\u001b[0m ]\n\u001b[1;32m    181\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39mexported)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"simulate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = hybrid_model.model.inference_model\n",
    "\n",
    "hybrid_model.set_fhe_mode(\"disable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE?\n",
      "\n",
      "FHE is a cryptographic technique that enables computations on arbitrary data structures. It consists in generating computable FAs\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with peft_model.disable_adapter_layers():\n",
    "    # Example usage\n",
    "    prompt = \"What is FHE ?\"\n",
    "    generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights_and_size(model, print_detail=False):\n",
    "    total_weights = 0\n",
    "    total_lora_weights = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total_weights += param.numel()\n",
    "\n",
    "        if \"lora\" in name:\n",
    "            total_lora_weights += param.numel()\n",
    "\n",
    "        if print_detail:\n",
    "            print(name, param.numel())\n",
    "\n",
    "    print(f\"Total number of weights: {total_weights}\")\n",
    "    print(f\"Total number of LoRA weights: {total_lora_weights}\")\n",
    "\n",
    "    return total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 124587264\n",
      "Total number of LoRA weights: 147456\n"
     ]
    }
   ],
   "source": [
    "total_weights_size = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"gpt2_lora_finetuned_hybrid_deployment\")\n",
    "\n",
    "if path.is_dir() and any(path.iterdir()):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "hybrid_model.save_and_clear_private_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 39569664\n"
     ]
    }
   ],
   "source": [
    "total_weights_size_private = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights removed: 68.24 %\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Total weights removed: \"\n",
    "    f\"{(total_weights_size - total_weights_size_private) / total_weights_size * 100:.2f} %\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
