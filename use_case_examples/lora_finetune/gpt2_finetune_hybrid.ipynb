{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from lora_module import LoraTraining\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from remote_module import CustomConv1D\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Conv1D,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Freeze weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=30, fhe=\"disable\"):\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FH: A basic program that is used to calculate the height of an object, and then sets the minimum height to be\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_conv1d(module, module_index_to_skip=0):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, Conv1D):\n",
    "\n",
    "            # Skip the module if the index has not been reached, and decrement the index\n",
    "            if module_index_to_skip >= 0:\n",
    "                module_index_to_skip -= 1\n",
    "            else:\n",
    "                custom_linear = CustomConv1D(child.weight, bias=child.bias)\n",
    "                setattr(module, name, custom_linear)\n",
    "        else:\n",
    "            module_index_to_skip = replace_conv1d(child, module_index_to_skip=module_index_to_skip)\n",
    "\n",
    "    return module_index_to_skip\n",
    "\n",
    "\n",
    "# Gradients of the first base layer that is used for fine-tuning are not needed. We\n",
    "# therefore need to exclude the backward module from the remote_names since calibration\n",
    "# won't get through it (which raises an issue with hybrid models)\n",
    "replace_conv1d(peft_model, module_index_to_skip=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "lora_training = LoraTraining(peft_model, GRADIENT_ACCUMULATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 128\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data_finetune/what_is_fhe.txt\",\n",
    "    block_size=BLOCK_SIZE,\n",
    "    cache_dir=\"cache_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "EPOCHS = 100\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=5e-4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=0,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = len_dataloader // training_args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "max_steps = math.ceil(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
    "\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "lora_training.update_training_parameters(trainer.optimizer, trainer.lr_scheduler, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remote_names(model):\n",
    "    remote_names = []\n",
    "    for name, module in model.named_modules():\n",
    "        # Some gradients are not needed for fine-tuning, so need to exclude the backward module\n",
    "        # from the remote_names since calibration won't get through it (which raises an issue with\n",
    "        # hybrid models). We however still need to include the associated module's forward pass in\n",
    "        # the hybrid model\n",
    "        if isinstance(module, Conv1D):\n",
    "            remote_names.append(name)\n",
    "\n",
    "        elif isinstance(module, CustomConv1D):\n",
    "            remote_names.append(name + \".forward_module\")\n",
    "            remote_names.append(name + \".backward_module\")\n",
    "\n",
    "    return remote_names\n",
    "\n",
    "\n",
    "remote_names = get_remote_names(lora_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = HybridFHEModel(lora_training, module_names=remote_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "label_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "\n",
    "inputset = (input_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.model.toggle_calibrate(enable=True)\n",
    "\n",
    "hybrid_model.compile_model(\n",
    "    inputset, n_bits=8, rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"}\n",
    ")\n",
    "\n",
    "hybrid_model.model.toggle_calibrate(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"disable\"):\n",
    "    device = \"cpu\"\n",
    "    hybrid_model.model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    hybrid_model.model.inference_model.train()\n",
    "\n",
    "    total_epochs = int(training_args.num_train_epochs)\n",
    "    epoch_pbar = tqdm(total=total_epochs, desc=\"Training Progress\", position=0)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        grad_norms = []\n",
    "\n",
    "        steps_in_epoch = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Gradient accumulation\n",
    "            is_last_batch_step = (\n",
    "                steps_in_epoch <= training_args.gradient_accumulation_steps\n",
    "                and (step + 1) == steps_in_epoch\n",
    "            )\n",
    "            accumulate_gradients = (\n",
    "                total_batched_samples % training_args.gradient_accumulation_steps == 0\n",
    "            )\n",
    "\n",
    "            run_optimizer = is_last_batch_step or accumulate_gradients\n",
    "\n",
    "            hybrid_model.model.toggle_run_optimizer(enable=run_optimizer)\n",
    "\n",
    "            loss, grad_norm = hybrid_model((batch[\"input_ids\"], batch[\"labels\"]), fhe=fhe)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if grad_norm is not None:\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = hybrid_model.model.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Get last grad norm\n",
    "        current_grad_norm = grad_norms[-1]\n",
    "\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, \"\n",
    "            f\"Loss: {total_loss:.4f}, grad norm: {current_grad_norm}, lr: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if training_args.output_dir is not None:\n",
    "        save_path = f\"{training_args.output_dir}/checkpoint-{epoch + 1}\"\n",
    "        hybrid_model.model.inference_model.save_pretrained(save_path)\n",
    "\n",
    "    epoch_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"simulate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = hybrid_model.model.inference_model\n",
    "\n",
    "hybrid_model.set_fhe_mode(\"simulate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE?\n",
      "\n",
      "FHE is a cryptographic technique that enables computations on arbitrary data structures. It consists in generating computable FAs\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.disable_adapter_layers()\n",
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)\n",
    "\n",
    "peft_model.enable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights_and_size(model, print_detail=False):\n",
    "    total_weights = 0\n",
    "    total_lora_weights = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total_weights += param.numel()\n",
    "\n",
    "        if \"lora\" in name:\n",
    "            total_lora_weights += param.numel()\n",
    "\n",
    "        if print_detail:\n",
    "            print(name, param.numel())\n",
    "\n",
    "    print(f\"Total number of weights: {total_weights}\")\n",
    "    print(f\"Total number of LoRA weights: {total_lora_weights}\")\n",
    "\n",
    "    return total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 124587264\n",
      "Total number of LoRA weights: 147456\n"
     ]
    }
   ],
   "source": [
    "total_weights_size = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"gpt2_lora_finetuned_hybrid_deployment\")\n",
    "\n",
    "if path.is_dir() and any(path.iterdir()):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "hybrid_model.save_and_clear_private_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 39569664\n"
     ]
    }
   ],
   "source": [
    "total_weights_size_private = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights removed: 68.24 %\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Total weights removed: \"\n",
    "    f\"{(total_weights_size - total_weights_size_private) / total_weights_size * 100:.2f} %\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
