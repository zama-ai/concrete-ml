{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from custom_module import CustomConv1D\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Conv1D,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# FREEZE WEIGHTS\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=30, fhe=\"disable\"):\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FH: A basic program that is used to calculate the height of an object, and then sets the minimum height to be\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_conv1d(module, module_index_to_skip=0):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, Conv1D):\n",
    "\n",
    "            # Skip the module if the index has not been reached, and decrement the index\n",
    "            if module_index_to_skip >= 0:\n",
    "                module_index_to_skip -= 1\n",
    "            else:\n",
    "                custom_linear = CustomConv1D(child.weight, bias=child.bias)\n",
    "                setattr(module, name, custom_linear)\n",
    "        else:\n",
    "            module_index_to_skip = replace_conv1d(child, module_index_to_skip=module_index_to_skip)\n",
    "\n",
    "    return module_index_to_skip\n",
    "\n",
    "\n",
    "# Gradients of the first base layer that is used for fine-tuning are not needed. We\n",
    "# therefore need to exclude the backward module from the remote_names since calibration\n",
    "# won't get through it (which raises an issue with hybrid models)\n",
    "replace_conv1d(peft_model, module_index_to_skip=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraTraining(torch.nn.Module):\n",
    "    def __init__(self, inference_model, gradient_accumulation_steps) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.inference_model = inference_model\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.lr_scheduler = None\n",
    "\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.max_grad_norm = None\n",
    "\n",
    "        self.calibrate = False\n",
    "        self.run_optimizer = False\n",
    "\n",
    "    def update_training_parameters(self, optimizer, lr_scheduler, training_args):\n",
    "        assert self.gradient_accumulation_steps == training_args.gradient_accumulation_steps\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.max_grad_norm = training_args.max_grad_norm\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # FIXME: handle multi-inputs in hybrid model\n",
    "        x, y = inputs\n",
    "\n",
    "        # some parts on server side\n",
    "        outputs = self.inference_model(input_ids=x, labels=y)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "        # Update gradients\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = None\n",
    "        if not self.calibrate and self.run_optimizer:\n",
    "            assert self.optimizer is not None\n",
    "            assert self.lr_scheduler is not None\n",
    "            assert self.max_grad_norm is not None\n",
    "\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                self.inference_model.parameters(), max_norm=self.max_grad_norm, norm_type=2\n",
    "            )\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "            self.inference_model.zero_grad()\n",
    "\n",
    "        # Clean gradients after calibration\n",
    "        elif self.calibrate:\n",
    "            self.inference_model.zero_grad()\n",
    "\n",
    "        return (loss, grad_norm)\n",
    "\n",
    "    def toggle_calibrate(self, enable: bool = True):\n",
    "        self.calibrate = enable\n",
    "\n",
    "    def toggle_run_optimizer(self, enable: bool = True):\n",
    "        self.run_optimizer = enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "lora_training = LoraTraining(peft_model, GRADIENT_ACCUMULATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 128\n",
    "\n",
    "\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=BLOCK_SIZE,\n",
    "        cache_dir=\"cache_dataset\",\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"data_finetune/what_is_fhe.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.parallelism = False\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "EPOCHS = 100\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=5e-4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=0,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = len_dataloader // training_args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "max_steps = math.ceil(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
    "\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_training.update_training_parameters(trainer.optimizer, trainer.lr_scheduler, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remote_names(model):\n",
    "    remote_names = []\n",
    "    for name, module in model.named_modules():\n",
    "        # Some gradients are not needed for fine-tuning, so need to exclude the backward module\n",
    "        # from the remote_names since calibration won't get through it (which raises an issue with\n",
    "        # hybrid models). We however still need to include the associated module's forward pass in\n",
    "        # the hybrid model\n",
    "        if isinstance(module, Conv1D):\n",
    "            remote_names.append(name)\n",
    "\n",
    "        elif isinstance(module, CustomConv1D):\n",
    "            remote_names.append(name + \".forward_module\")\n",
    "            remote_names.append(name + \".backward_module\")\n",
    "\n",
    "    return remote_names\n",
    "\n",
    "\n",
    "remote_names = get_remote_names(lora_training)\n",
    "\n",
    "hybrid_model = HybridFHEModel(lora_training, module_names=remote_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "label_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (\n",
    "    tokenizer.vocab_size - 1\n",
    ")\n",
    "\n",
    "inputset = (input_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.model.toggle_calibrate(enable=True)\n",
    "\n",
    "hybrid_model.compile_model(\n",
    "    inputset, n_bits=8, rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"}\n",
    ")\n",
    "\n",
    "hybrid_model.model.toggle_calibrate(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"disable\"):\n",
    "    device = \"cpu\"\n",
    "    hybrid_model.model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    hybrid_model.model.inference_model.train()\n",
    "\n",
    "    total_epochs = int(training_args.num_train_epochs)\n",
    "    epoch_pbar = tqdm(total=total_epochs, desc=\"Training Progress\", position=0)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        grad_norms = []\n",
    "\n",
    "        steps_in_epoch = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Gradient accumulation\n",
    "            is_last_batch_step = (\n",
    "                steps_in_epoch <= training_args.gradient_accumulation_steps\n",
    "                and (step + 1) == steps_in_epoch\n",
    "            )\n",
    "            accumulate_gradients = (\n",
    "                total_batched_samples % training_args.gradient_accumulation_steps == 0\n",
    "            )\n",
    "\n",
    "            run_optimizer = is_last_batch_step or accumulate_gradients\n",
    "\n",
    "            hybrid_model.model.toggle_run_optimizer(enable=run_optimizer)\n",
    "\n",
    "            loss, grad_norm = hybrid_model((batch[\"input_ids\"], batch[\"labels\"]), fhe=fhe)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if grad_norm is not None:\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = hybrid_model.model.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Get last grad norm\n",
    "        current_grad_norm = grad_norms[-1]\n",
    "\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, \"\n",
    "            f\"Loss: {total_loss:.4f}, grad norm: {current_grad_norm}, lr: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if training_args.output_dir is not None:\n",
    "        save_path = f\"{training_args.output_dir}/checkpoint-{epoch + 1}\"\n",
    "        hybrid_model.model.inference_model.save_pretrained(save_path)\n",
    "\n",
    "    epoch_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   1%|          | 1/100 [00:01<01:56,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.5326, grad norm: 0.6547388434410095, lr: 0.000495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   2%|▏         | 2/100 [00:01<01:10,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 1.5092, grad norm: 0.5734729170799255, lr: 0.00049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   3%|▎         | 3/100 [00:01<00:54,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 1.4762, grad norm: 0.4197540581226349, lr: 0.00048499999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   4%|▍         | 4/100 [00:02<00:47,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 1.5085, grad norm: 0.569969117641449, lr: 0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   5%|▌         | 5/100 [00:02<00:42,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 1.4667, grad norm: 0.5897998213768005, lr: 0.000475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   6%|▌         | 6/100 [00:03<00:40,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 1.4486, grad norm: 0.44352057576179504, lr: 0.00047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   7%|▋         | 7/100 [00:03<00:38,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 1.4159, grad norm: 0.506279706954956, lr: 0.000465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   8%|▊         | 8/100 [00:03<00:37,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 1.4051, grad norm: 0.6538838148117065, lr: 0.00046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:   9%|▉         | 9/100 [00:04<00:36,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 1.3889, grad norm: 0.6592888236045837, lr: 0.000455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  10%|█         | 10/100 [00:04<00:35,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.3730, grad norm: 0.6219719052314758, lr: 0.00045000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  11%|█         | 11/100 [00:05<00:34,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 1.3494, grad norm: 0.6324566602706909, lr: 0.00044500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  12%|█▏        | 12/100 [00:05<00:34,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 1.3169, grad norm: 0.5421789288520813, lr: 0.00044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  13%|█▎        | 13/100 [00:05<00:34,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 1.3013, grad norm: 0.5423504114151001, lr: 0.000435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  14%|█▍        | 14/100 [00:06<00:33,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 1.3303, grad norm: 0.6302087903022766, lr: 0.00043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  15%|█▌        | 15/100 [00:06<00:32,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 1.2771, grad norm: 0.5095004439353943, lr: 0.000425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  16%|█▌        | 16/100 [00:06<00:32,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 1.2506, grad norm: 0.5400538444519043, lr: 0.00042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  17%|█▋        | 17/100 [00:07<00:31,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 1.2341, grad norm: 0.5874373316764832, lr: 0.000415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  18%|█▊        | 18/100 [00:07<00:31,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 1.2215, grad norm: 0.5731167793273926, lr: 0.00041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  19%|█▉        | 19/100 [00:08<00:30,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 1.1856, grad norm: 0.5122016072273254, lr: 0.00040500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  20%|██        | 20/100 [00:08<00:30,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 1.1938, grad norm: 0.5971183180809021, lr: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  21%|██        | 21/100 [00:08<00:30,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 1.1668, grad norm: 0.6376621127128601, lr: 0.000395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  22%|██▏       | 22/100 [00:09<00:29,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 1.1436, grad norm: 0.5452390909194946, lr: 0.00039000000000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  23%|██▎       | 23/100 [00:09<00:29,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 1.1361, grad norm: 0.5471293330192566, lr: 0.00038500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  24%|██▍       | 24/100 [00:10<00:29,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 1.1000, grad norm: 0.6130229234695435, lr: 0.00038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  25%|██▌       | 25/100 [00:10<00:28,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 1.0795, grad norm: 0.6525614261627197, lr: 0.000375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  26%|██▌       | 26/100 [00:10<00:28,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 1.0930, grad norm: 0.9915198683738708, lr: 0.00037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  27%|██▋       | 27/100 [00:11<00:28,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 1.0531, grad norm: 0.590857207775116, lr: 0.000365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  28%|██▊       | 28/100 [00:11<00:28,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 1.0564, grad norm: 0.8754357695579529, lr: 0.00035999999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  29%|██▉       | 29/100 [00:11<00:27,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 1.0520, grad norm: 0.8149130344390869, lr: 0.000355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  30%|███       | 30/100 [00:12<00:27,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 1.0313, grad norm: 0.5920228958129883, lr: 0.00035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  31%|███       | 31/100 [00:12<00:26,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 1.0182, grad norm: 0.6779032349586487, lr: 0.000345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  32%|███▏      | 32/100 [00:13<00:26,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 0.9980, grad norm: 0.5544361472129822, lr: 0.00034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  33%|███▎      | 33/100 [00:13<00:26,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 0.9932, grad norm: 0.7196674942970276, lr: 0.000335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  34%|███▍      | 34/100 [00:13<00:25,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 0.9819, grad norm: 0.7083548903465271, lr: 0.00033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  35%|███▌      | 35/100 [00:14<00:24,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 0.9250, grad norm: 0.7313346266746521, lr: 0.00032500000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  36%|███▌      | 36/100 [00:14<00:24,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 0.9198, grad norm: 0.6564635634422302, lr: 0.00032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  37%|███▋      | 37/100 [00:15<00:23,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 0.9157, grad norm: 0.7937288880348206, lr: 0.000315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  38%|███▊      | 38/100 [00:15<00:23,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 0.8932, grad norm: 0.6338443756103516, lr: 0.00031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  39%|███▉      | 39/100 [00:15<00:23,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 0.9295, grad norm: 0.8935690522193909, lr: 0.000305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  40%|████      | 40/100 [00:16<00:22,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.8730, grad norm: 0.7592346668243408, lr: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  41%|████      | 41/100 [00:16<00:22,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 0.8485, grad norm: 0.7101594805717468, lr: 0.000295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  42%|████▏     | 42/100 [00:16<00:21,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 0.8411, grad norm: 0.6478201150894165, lr: 0.00029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  43%|████▎     | 43/100 [00:17<00:21,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 0.8544, grad norm: 0.7164880037307739, lr: 0.000285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  44%|████▍     | 44/100 [00:17<00:21,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 0.8414, grad norm: 0.7436962127685547, lr: 0.00028000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  45%|████▌     | 45/100 [00:18<00:20,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Loss: 0.8121, grad norm: 0.9844059944152832, lr: 0.000275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  46%|████▌     | 46/100 [00:18<00:20,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Loss: 0.8048, grad norm: 0.9871523976325989, lr: 0.00027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  47%|████▋     | 47/100 [00:18<00:20,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Loss: 0.8153, grad norm: 0.8394853472709656, lr: 0.00026500000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  48%|████▊     | 48/100 [00:19<00:19,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Loss: 0.8046, grad norm: 0.9217925667762756, lr: 0.00026000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  49%|████▉     | 49/100 [00:19<00:19,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Loss: 0.7614, grad norm: 1.015302062034607, lr: 0.000255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  50%|█████     | 50/100 [00:19<00:19,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 0.7760, grad norm: 0.9043252468109131, lr: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  51%|█████     | 51/100 [00:20<00:18,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Loss: 0.7693, grad norm: 0.8068227767944336, lr: 0.000245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  52%|█████▏    | 52/100 [00:20<00:18,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Loss: 0.7422, grad norm: 0.9263298511505127, lr: 0.00024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  53%|█████▎    | 53/100 [00:21<00:17,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Loss: 0.7486, grad norm: 1.0840318202972412, lr: 0.000235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  54%|█████▍    | 54/100 [00:21<00:17,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Loss: 0.7469, grad norm: 0.8277450799942017, lr: 0.00023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  55%|█████▌    | 55/100 [00:21<00:17,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Loss: 0.7148, grad norm: 0.8486602306365967, lr: 0.00022500000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  56%|█████▌    | 56/100 [00:22<00:16,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Loss: 0.7018, grad norm: 0.9315493106842041, lr: 0.00022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  57%|█████▋    | 57/100 [00:22<00:16,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Loss: 0.6978, grad norm: 0.8715642690658569, lr: 0.000215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  58%|█████▊    | 58/100 [00:23<00:16,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Loss: 0.6954, grad norm: 0.9117729067802429, lr: 0.00021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  59%|█████▉    | 59/100 [00:23<00:15,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Loss: 0.6805, grad norm: 0.8932844996452332, lr: 0.000205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  60%|██████    | 60/100 [00:23<00:15,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 0.6801, grad norm: 1.0779385566711426, lr: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  61%|██████    | 61/100 [00:24<00:14,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Loss: 0.6582, grad norm: 0.9519742131233215, lr: 0.00019500000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  62%|██████▏   | 62/100 [00:24<00:14,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Loss: 0.6777, grad norm: 1.0926264524459839, lr: 0.00019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  63%|██████▎   | 63/100 [00:24<00:14,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Loss: 0.6813, grad norm: 1.2714309692382812, lr: 0.000185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  64%|██████▍   | 64/100 [00:25<00:13,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Loss: 0.6696, grad norm: 1.0693631172180176, lr: 0.00017999999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  65%|██████▌   | 65/100 [00:25<00:13,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Loss: 0.6621, grad norm: 1.1618248224258423, lr: 0.000175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  66%|██████▌   | 66/100 [00:26<00:13,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Loss: 0.6314, grad norm: 0.9860178232192993, lr: 0.00017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  67%|██████▋   | 67/100 [00:26<00:12,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Loss: 0.6267, grad norm: 1.0095081329345703, lr: 0.000165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  68%|██████▊   | 68/100 [00:26<00:12,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Loss: 0.6275, grad norm: 0.9747483134269714, lr: 0.00016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  69%|██████▉   | 69/100 [00:27<00:12,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Loss: 0.6391, grad norm: 1.1718988418579102, lr: 0.000155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  70%|███████   | 70/100 [00:27<00:11,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 0.6172, grad norm: 0.8902360796928406, lr: 0.00015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  71%|███████   | 71/100 [00:28<00:11,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Loss: 0.6180, grad norm: 1.0743216276168823, lr: 0.000145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  72%|███████▏  | 72/100 [00:28<00:10,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Loss: 0.6124, grad norm: 1.4731453657150269, lr: 0.00014000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  73%|███████▎  | 73/100 [00:28<00:10,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Loss: 0.5906, grad norm: 1.2012979984283447, lr: 0.000135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  74%|███████▍  | 74/100 [00:29<00:10,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Loss: 0.5892, grad norm: 1.3028196096420288, lr: 0.00013000000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  75%|███████▌  | 75/100 [00:29<00:09,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Loss: 0.5887, grad norm: 1.0304925441741943, lr: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  76%|███████▌  | 76/100 [00:30<00:09,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Loss: 0.5687, grad norm: 0.9565426707267761, lr: 0.00012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  77%|███████▋  | 77/100 [00:30<00:09,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Loss: 0.5913, grad norm: 1.1523699760437012, lr: 0.000115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  78%|███████▊  | 78/100 [00:30<00:08,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Loss: 0.5948, grad norm: 1.1738296747207642, lr: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  79%|███████▉  | 79/100 [00:31<00:08,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Loss: 0.5651, grad norm: 1.260327696800232, lr: 0.000105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  80%|████████  | 80/100 [00:31<00:08,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 0.5798, grad norm: 1.1174153089523315, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  81%|████████  | 81/100 [00:32<00:07,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Loss: 0.5539, grad norm: 0.9862734079360962, lr: 9.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  82%|████████▏ | 82/100 [00:32<00:07,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 0.5794, grad norm: 0.9966534972190857, lr: 8.999999999999999e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  83%|████████▎ | 83/100 [00:32<00:06,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Loss: 0.5628, grad norm: 1.0192633867263794, lr: 8.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  84%|████████▍ | 84/100 [00:33<00:06,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Loss: 0.5633, grad norm: 0.9687011241912842, lr: 8e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  85%|████████▌ | 85/100 [00:33<00:05,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Loss: 0.5684, grad norm: 1.0944551229476929, lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  86%|████████▌ | 86/100 [00:33<00:05,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Loss: 0.5491, grad norm: 1.0738519430160522, lr: 7.000000000000001e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  87%|████████▋ | 87/100 [00:34<00:04,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Loss: 0.5521, grad norm: 1.282754898071289, lr: 6.500000000000001e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  88%|████████▊ | 88/100 [00:34<00:04,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Loss: 0.5486, grad norm: 0.9802149534225464, lr: 6e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  89%|████████▉ | 89/100 [00:35<00:03,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Loss: 0.5413, grad norm: 1.0144387483596802, lr: 5.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  90%|█████████ | 90/100 [00:35<00:03,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Loss: 0.5474, grad norm: 1.223002552986145, lr: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  91%|█████████ | 91/100 [00:35<00:03,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Loss: 0.5436, grad norm: 1.1522656679153442, lr: 4.4999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  92%|█████████▏| 92/100 [00:36<00:02,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Loss: 0.5376, grad norm: 1.1914536952972412, lr: 4e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  93%|█████████▎| 93/100 [00:36<00:02,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Loss: 0.5398, grad norm: 1.0207066535949707, lr: 3.5000000000000004e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  94%|█████████▍| 94/100 [00:36<00:02,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Loss: 0.5116, grad norm: 1.0995105504989624, lr: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  95%|█████████▌| 95/100 [00:37<00:01,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Loss: 0.5242, grad norm: 1.0830743312835693, lr: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  96%|█████████▌| 96/100 [00:37<00:01,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Loss: 0.5603, grad norm: 1.2351734638214111, lr: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  97%|█████████▋| 97/100 [00:37<00:01,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Loss: 0.5152, grad norm: 0.9580557346343994, lr: 1.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  98%|█████████▊| 98/100 [00:38<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Loss: 0.5217, grad norm: 0.9174291491508484, lr: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress:  99%|█████████▉| 99/100 [00:38<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Loss: 0.5333, grad norm: 1.0415540933609009, lr: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress: 100%|██████████| 100/100 [00:38<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.5352, grad norm: 0.9800519943237305, lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training Progress: 100%|██████████| 100/100 [00:38<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"disable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = hybrid_model.model.inference_model\n",
    "\n",
    "hybrid_model.set_fhe_mode(\"disable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE?\n",
      "\n",
      "FHE is a cryptographic technique that enables computations on arbitrary data structures. It consists in generating computable FAs\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with peft_model.disable_adapter_layers():\n",
    "    # Example usage\n",
    "    prompt = \"What is FHE ?\"\n",
    "    generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights_and_size(model, print_detail=False):\n",
    "    total_weights = 0\n",
    "    total_lora_weights = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total_weights += param.numel()\n",
    "\n",
    "        if \"lora\" in name:\n",
    "            total_lora_weights += param.numel()\n",
    "\n",
    "        if print_detail:\n",
    "            print(name, param.numel())\n",
    "\n",
    "    print(f\"Total number of weights: {total_weights}\")\n",
    "    print(f\"Total number of LoRA weights: {total_lora_weights}\")\n",
    "\n",
    "    return total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 124587264\n",
      "Total number of LoRA weights: 147456\n"
     ]
    }
   ],
   "source": [
    "total_weights_size = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"gpt2_lora_finetuned_hybrid_deployment\")\n",
    "\n",
    "if path.is_dir() and any(path.iterdir()):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "hybrid_model.save_and_clear_private_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 39569664\n"
     ]
    }
   ],
   "source": [
    "total_weights_size_private = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights removed: 68.24 %\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Total weights removed: \"\n",
    "    f\"{(total_weights_size - total_weights_size_private) / total_weights_size * 100:.2f} %\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
