{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling, Conv1D, TextDataset, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "from lora_fhe_module import CustomConv1D\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# FREEZE WEIGHTS\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=30, fhe=\"disable\"):\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? FH: A basic program that is used to calculate the height of an object, and then sets the minimum height to be\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who's Barack Obama?\n",
      "\n",
      "I think he's very much in a position to be president. I mean, he was nominated by his party.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Who's Barack Obama ?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_conv1d(module, module_index_to_skip=0):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, Conv1D):\n",
    "\n",
    "            # Skip the module if the index has not been reached, and decrement the index\n",
    "            if module_index_to_skip >= 0:\n",
    "                module_index_to_skip -= 1\n",
    "            else:\n",
    "                custom_linear = CustomConv1D(child.weight, bias=child.bias)\n",
    "                setattr(module, name, custom_linear)\n",
    "        else:\n",
    "            module_index_to_skip = replace_conv1d(child, module_index_to_skip=module_index_to_skip)\n",
    "    \n",
    "    return module_index_to_skip\n",
    "\n",
    "# Gradients of the first base layer that is used for fine-tuning are not needed. We\n",
    "# therefore need to exclude the backward module from the remote_names since calibration \n",
    "# won't get through it (which raises an issue with hybrid models)\n",
    "replace_conv1d(peft_model, module_index_to_skip=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraTraining(torch.nn.Module):\n",
    "    def __init__(self, inference_model, gradient_accumulation_steps) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inference_model = inference_model\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.lr_scheduler = None\n",
    "    \n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.max_grad_norm = None\n",
    "        \n",
    "        self.calibrate = False\n",
    "        self.run_optimizer = False\n",
    "    \n",
    "    def update_training_parameters(self, optimizer, lr_scheduler, training_args):\n",
    "        assert self.gradient_accumulation_steps == training_args.gradient_accumulation_steps\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.max_grad_norm = training_args.max_grad_norm\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # FIXME: handle multi-inputs in hybrid model\n",
    "        x, y = inputs\n",
    "            \n",
    "        # some parts on server side \n",
    "        outputs = self.inference_model(input_ids=x, labels=y)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss = loss / self.gradient_accumulation_steps\n",
    "        \n",
    "        # Update gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = None\n",
    "        if not self.calibrate and self.run_optimizer:\n",
    "            assert self.optimizer is not None\n",
    "            assert self.lr_scheduler is not None\n",
    "            assert self.max_grad_norm is not None\n",
    "            \n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.inference_model.parameters(), max_norm=self.max_grad_norm, norm_type=2)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "            \n",
    "            self.inference_model.zero_grad()\n",
    "        \n",
    "        # Clean gradients after calibration\n",
    "        elif self.calibrate:\n",
    "            self.inference_model.zero_grad()\n",
    "        \n",
    "        return (loss, grad_norm)\n",
    "    \n",
    "    def toggle_calibrate(self, enable: bool = True):\n",
    "        self.calibrate = enable\n",
    "        \n",
    "    def toggle_run_optimizer(self, enable: bool = True):\n",
    "        self.run_optimizer = enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "lora_training = LoraTraining(peft_model, GRADIENT_ACCUMULATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 128\n",
    "\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=BLOCK_SIZE,\n",
    "        cache_dir=\"cache_dataset\",\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = load_dataset(\"data_finetune/what_is_fhe.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.parallelism = False\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "EPOCHS = 100\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=5e-4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=0,\n",
    "    max_grad_norm=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = len_dataloader // training_args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "max_steps = math.ceil(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
    "\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_training.update_training_parameters(trainer.optimizer, trainer.lr_scheduler, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remote_names(model):\n",
    "    remote_names = []\n",
    "    for name, module in model.named_modules():\n",
    "        # Some gradients are not needed for fine-tuning, so need to exclude the backward module \n",
    "        # from the remote_names since calibration won't get through it (which raises an issue with \n",
    "        # hybrid models). We however still need to include the associated module's forward pass in \n",
    "        # the hybrid model\n",
    "        if isinstance(module, Conv1D):\n",
    "            remote_names.append(name)\n",
    "        \n",
    "        elif isinstance(module, CustomConv1D):\n",
    "            remote_names.append(name + \".forward_module\")\n",
    "            remote_names.append(name + \".backward_module\")\n",
    "            \n",
    "    return remote_names\n",
    "\n",
    "remote_names = get_remote_names(lora_training)\n",
    "\n",
    "hybrid_model = HybridFHEModel(lora_training, module_names=remote_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (tokenizer.vocab_size - 1)\n",
    "label_tensor = torch.randint(0, 2, (PER_DEVICE_TRAIN_BATCH_SIZE, BLOCK_SIZE)) * (tokenizer.vocab_size - 1)\n",
    "\n",
    "inputset = (input_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.model.toggle_calibrate(enable=True)\n",
    "\n",
    "hybrid_model.compile_model(inputset, n_bits=8, rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"}, p_error=1e-5)\n",
    "\n",
    "hybrid_model.model.toggle_calibrate(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"disable\"):\n",
    "    device = \"cpu\"\n",
    "    hybrid_model.model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    hybrid_model.model.inference_model.train()\n",
    " \n",
    "    total_epochs = int(training_args.num_train_epochs)\n",
    "    epoch_pbar = tqdm(total=total_epochs, desc=\"Training Progress\", position=0)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        grad_norms = []\n",
    "        \n",
    "        steps_in_epoch = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            total_batched_samples+=1\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            is_last_batch_step = steps_in_epoch <= training_args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n",
    "            accumulate_gradients = (total_batched_samples % training_args.gradient_accumulation_steps == 0)\n",
    "            \n",
    "            run_optimizer = is_last_batch_step or accumulate_gradients\n",
    "            \n",
    "            hybrid_model.model.toggle_run_optimizer(enable=run_optimizer)\n",
    "            \n",
    "            loss, grad_norm = hybrid_model((batch['input_ids'], batch['labels']), fhe=fhe)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if grad_norm is not None:\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = hybrid_model.model.lr_scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Get last grad norm\n",
    "        current_grad_norm = grad_norms[-1]\n",
    "\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, \"\n",
    "            f\"Loss: {total_loss:.4f}, grad norm: {current_grad_norm}, lr: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "        \n",
    "    # Save model checkpoint\n",
    "    if training_args.output_dir is not None:\n",
    "        save_path = f\"{training_args.output_dir}/checkpoint-{epoch + 1}\"\n",
    "        hybrid_model.model.inference_model.save_pretrained(save_path)\n",
    "            \n",
    "    epoch_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Progress:  50%|█████     | 1/2 [02:29<02:29, 149.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 1.8521, grad norm: 0.29645875096321106, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 2/2 [04:58<00:00, 149.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 1.8171, grad norm: 0.21446043252944946, lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 2/2 [04:58<00:00, 149.21s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_custom_model(hybrid_model, train_dataloader, training_args, fhe=\"disable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = hybrid_model.model.inference_model\n",
    "\n",
    "hybrid_model.set_fhe_mode(\"disable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE?\n",
      "\n",
      "He is\n",
      "HE is not\n",
      ".\n",
      " (This is a not. HE\n",
      "I\n",
      "It is NOT.\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model.disable_adapter_layers()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is FHE ?\" \n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)\n",
    "\n",
    "fine_tuned_model.enable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who's Barack Obama? I have an idea for the Obama, I think that is I can't even imagine. But let me just say, it\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Who's Barack Obama ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? I don't know. I'm just a big big fat fat.\n",
      "\n",
      "I have no idea, but I do not\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE? F1 was FH is always F I think Fhehe F he F He He is, He, is he,\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model.disable_adapter_layers()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FHE?\n",
      "\n",
      "You're probably thinking of a few different ways\n",
      "As far as I'm thinking about it's not really that different\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model.enable_adapter_layers()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is FHE ?\"\n",
    "generated_text = generate_text(prompt, fine_tuned_model, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights_and_size(model, print_detail=False):\n",
    "    total_weights = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total_weights += param.numel()\n",
    "        if print_detail:\n",
    "            print(name, param.numel())\n",
    "    \n",
    "    print(f\"Total number of weights: {total_weights}\")\n",
    "    \n",
    "    return total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 124587264\n"
     ]
    }
   ],
   "source": [
    "total_weights_size = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"gpt2_lora_finetuned_hybrid_deployment\")\n",
    "\n",
    "if (path.is_dir() and any(path.iterdir())):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "hybrid_model.save_and_clear_private_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weights: 39569664\n"
     ]
    }
   ],
   "source": [
    "total_weights_size_private = print_weights_and_size(hybrid_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights removed: 68.24 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weights removed: {(total_weights_size - total_weights_size_private) / total_weights_size * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
