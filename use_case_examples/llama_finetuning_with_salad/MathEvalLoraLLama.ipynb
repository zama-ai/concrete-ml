{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fc3f67",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA with LoRA for Math Word Problems\n",
    "\n",
    "This notebook demonstrates how to fine-tune the LLaMA-3.2-1B model using LoRA (Low-Rank Adaptation) on the Orca Math Word Problems dataset. We support multiple modes (Torch, 8-bit, 16-bit) and include evaluation metrics like perplexity.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb91ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from utils_lora import generate_and_print\n",
    "\n",
    "from concrete.ml.torch.lora import LoraTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db3207",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Define the training mode and other parameters here. Modify these values as needed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf52256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: 7bit, Eval Steps: 100, Force CPU: False\n"
     ]
    }
   ],
   "source": [
    "# Training mode: 'torch', '7bit', or '16bit'\n",
    "MODE = \"7bit\"\n",
    "\n",
    "# Number of steps between evaluations\n",
    "EVAL_STEPS = 100\n",
    "\n",
    "# Set to True to force CPU\n",
    "FORCE_CPU = False\n",
    "\n",
    "# File paths based on mode\n",
    "mode_str = MODE\n",
    "EVAL_RESPONSES_FILE = f\"eval_generated_responses_{mode_str}.txt\"\n",
    "TRAIN_LOG_FILE = f\"training_log_{mode_str}.txt\"\n",
    "SAVE_PATH = Path(f\"deployment/llama_lora_finetuned_{mode_str}\")\n",
    "\n",
    "print(f\"Mode: {MODE}, Eval Steps: {EVAL_STEPS}, Force CPU: {FORCE_CPU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7aca0",
   "metadata": {},
   "source": [
    "## Device and Seed Configuration\n",
    "\n",
    "Set up the device (CPU/GPU/MPS) and random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83416a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if FORCE_CPU:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The random seed to use\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "DEVICE = get_device()\n",
    "set_seed(SEED)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a064520",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the LLaMA model and tokenizer, and test the base model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab290ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: I have a problem with my math. I know that 7 is the answer, but I can't figure out what number it is. I am\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "PROMPT = \"When you multiply a number by 7, it becomes 98. What is that number?\\n\"\n",
    "_ = generate_and_print(PROMPT, model, tokenizer, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855c901",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Set up LoRA parameters and apply them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e83963",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293bc07",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Load the dataset, filter by length, and preprocess for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0373b27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length Distribution Statistics:\n",
      "Original dataset size: 200,035\n",
      "Filtered dataset size: 1,557\n",
      "Percentage kept: 0.8%\n",
      "\n",
      "Question lengths: \n",
      "  Min: 4, Max: 61\n",
      "  Mean: 24.7\n",
      "  Median: 25\n",
      "\n",
      "Answer lengths:\n",
      "  Min: 1, Max: 55\n",
      "  Mean: 23.5\n",
      "  Median: 23\n",
      "\n",
      "Total lengths (including newline):\n",
      "  Min: 6, Max: 64\n",
      "  Mean: 49.2\n",
      "  Median: 51\n",
      "\n",
      "Train samples: 1479, Test samples: 78\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 64\n",
    "raw_dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train\")\n",
    "\n",
    "\n",
    "def length_filter(example):\n",
    "    q_len = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    a_len = len(tokenizer(example[\"answer\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    return (q_len + a_len + 1) <= MAX_LENGTH\n",
    "\n",
    "\n",
    "filtered_dataset = raw_dataset.filter(length_filter)\n",
    "\n",
    "\n",
    "def get_lengths(example):\n",
    "    q_len = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    a_len = len(tokenizer(example[\"answer\"], add_special_tokens=False)[\"input_ids\"])\n",
    "    total_len = q_len + a_len + 1\n",
    "    return {\"q_len\": q_len, \"a_len\": a_len, \"total_len\": total_len}\n",
    "\n",
    "\n",
    "lengths = filtered_dataset.map(get_lengths)\n",
    "q_lengths = [x[\"q_len\"] for x in lengths]\n",
    "a_lengths = [x[\"a_len\"] for x in lengths]\n",
    "total_lengths = [x[\"total_len\"] for x in lengths]\n",
    "\n",
    "print(\"\\nLength Distribution Statistics:\")\n",
    "print(f\"Original dataset size: {len(raw_dataset):,}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset):,}\")\n",
    "print(f\"Percentage kept: {100 * len(filtered_dataset)/len(raw_dataset):.1f}%\\n\")\n",
    "print(\"Question lengths: \")\n",
    "print(f\"  Min: {min(q_lengths)}, Max: {max(q_lengths)}\")\n",
    "print(f\"  Mean: {sum(q_lengths)/len(q_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(q_lengths)[len(q_lengths)//2]}\")\n",
    "print(\"\\nAnswer lengths:\")\n",
    "print(f\"  Min: {min(a_lengths)}, Max: {max(a_lengths)}\")\n",
    "print(f\"  Mean: {sum(a_lengths)/len(a_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(a_lengths)[len(a_lengths)//2]}\")\n",
    "print(\"\\nTotal lengths (including newline):\")\n",
    "print(f\"  Min: {min(total_lengths)}, Max: {max(total_lengths)}\")\n",
    "print(f\"  Mean: {sum(total_lengths)/len(total_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(total_lengths)[len(total_lengths)//2]}\\n\")\n",
    "\n",
    "\n",
    "def process_example(example):\n",
    "    \"\"\"Tokenize a question-answer pair and prepare labels for training.\n",
    "\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'answer' strings\n",
    "    Returns:\n",
    "        dict: Processed tokens with masked labels for the question portion\n",
    "    \"\"\"\n",
    "    question = example[\"question\"].strip()\n",
    "    answer = example[\"answer\"].strip()\n",
    "    tokens = tokenizer(\n",
    "        question + \"\\n\" + answer,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    question_length = len(tokenizer(question, add_special_tokens=False)[\"input_ids\"]) + 1\n",
    "    labels = tokens[\"input_ids\"].copy()\n",
    "    for i in range(question_length):\n",
    "        if i < len(labels):\n",
    "            labels[i] = -100\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_dataset = filtered_dataset.map(\n",
    "    process_example,\n",
    "    batched=False,\n",
    "    remove_columns=filtered_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized = tokenized_dataset.train_test_split(test_size=0.05, seed=SEED, shuffle=True)\n",
    "train_dataset, test_dataset = tokenized[\"train\"], tokenized[\"test\"]\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b530ff",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "Define a custom data collator to handle padding while preserving label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b120a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        inputs, attention_masks, labels = [], [], []\n",
    "        max_length = max(len(example[\"input_ids\"]) for example in examples)\n",
    "        for example in examples:\n",
    "            inputs.append(example[\"input_ids\"])\n",
    "            attention_masks.append(example[\"attention_mask\"])\n",
    "            labels.append(example[\"labels\"])\n",
    "\n",
    "        def pad(sequences, value):\n",
    "            return [x + [value] * (max_length - len(x)) for x in sequences]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(pad(inputs, self.pad_id)),\n",
    "            \"attention_mask\": torch.tensor(pad(attention_masks, 0)),\n",
    "            \"labels\": torch.tensor(pad(labels, -100)),\n",
    "        }\n",
    "\n",
    "\n",
    "collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf397552",
   "metadata": {},
   "source": [
    "## Training Arguments\n",
    "\n",
    "Configure the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db068dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a8346",
   "metadata": {},
   "source": [
    "## Loss and Evaluation Metrics\n",
    "\n",
    "Define the loss function and evaluation metric (perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6eb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_lm_loss(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    return F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=-100,\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "\n",
    "\n",
    "def metric_fn(model, dataloader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    total_loss, total_tokens, results = 0.0, 0, []\n",
    "    response = generate_and_print(PROMPT, model, tokenizer, seed=SEED)\n",
    "    if response:\n",
    "        results.append({\"prompt\": PROMPT, \"response\": response})\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            batch_labels = batch[\"labels\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "            valid = batch_labels[..., 1:] != -100\n",
    "            loss = F.cross_entropy(\n",
    "                outputs[..., :-1, :].contiguous().view(-1, outputs.size(-1)),\n",
    "                batch_labels[..., 1:].contiguous().view(-1),\n",
    "                ignore_index=-100,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += valid.sum().item()\n",
    "    perplexity = math.exp(total_loss / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "    with open(EVAL_RESPONSES_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Perplexity: {perplexity:.2f}\\n\")\n",
    "        for i, r in enumerate(results):\n",
    "            f.write(\n",
    "                f\"== Generation {i+1} ==\\nPrompt:\\n{r['prompt']}\\n\\nResponse:\\n{r['response']}\\n\"\n",
    "            )\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d167837",
   "metadata": {},
   "source": [
    "## Trainer Setup\n",
    "\n",
    "Initialize the Hugging Face Trainer to set up the optimizer and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c8e7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "train_dl = hf_trainer.get_train_dataloader()\n",
    "hf_trainer.create_optimizer_and_scheduler(len(train_dl) * training_args.num_train_epochs)\n",
    "optimizer, lr_scheduler = hf_trainer.optimizer, hf_trainer.lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c048d",
   "metadata": {},
   "source": [
    "## Calibration and Evaluation Data\n",
    "\n",
    "Prepare dummy calibration data and the evaluation DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "face520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputset():\n",
    "    return {\n",
    "        \"input_ids\": torch.randint(0, tokenizer.vocab_size, (4, MAX_LENGTH)),\n",
    "        \"attention_mask\": torch.ones((4, MAX_LENGTH), dtype=torch.long),\n",
    "        \"labels\": torch.randint(0, tokenizer.vocab_size, (4, MAX_LENGTH)),\n",
    "    }\n",
    "\n",
    "\n",
    "eval_dl = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afbe6e",
   "metadata": {},
   "source": [
    "## Model Initialization Function\n",
    "\n",
    "Add a function to initialize models with consistent weights for our comparison experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de029b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(seed=SEED):\n",
    "    \"\"\"Initialize the model with consistent weights.\"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.01,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=\"all-linear\",\n",
    "    )\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model = peft_model.to(DEVICE)\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def setup_trainer(model, steps=EVAL_STEPS, logging_steps=1):\n",
    "    \"\"\"Set up the trainer, optimizer and scheduler.\"\"\"\n",
    "    hf_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    train_dl = hf_trainer.get_train_dataloader()\n",
    "    hf_trainer.create_optimizer_and_scheduler(len(train_dl) * training_args.num_train_epochs)\n",
    "    optimizer, lr_scheduler = hf_trainer.optimizer, hf_trainer.lr_scheduler\n",
    "\n",
    "    lora_trainer = LoraTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=causal_lm_loss,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        training_args=vars(training_args),\n",
    "        n_layers_to_skip_for_backprop=3,\n",
    "        eval_loader=eval_dl,\n",
    "        eval_metric_fn=metric_fn,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=steps,\n",
    "        train_log_path=TRAIN_LOG_FILE,\n",
    "\n",
    "        optimized_linear_execution=True,\n",
    "        server_remote_address=\"http://0.0.0.0:8000\",\n",
    "        model_name=f\"meta-llama\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    if MODE != \"torch\":\n",
    "        bits = 7 if MODE == \"7bit\" else 16\n",
    "        lora_trainer.compile(get_inputset(), n_bits=bits)\n",
    "\n",
    "    return lora_trainer, train_dl\n",
    "\n",
    "\n",
    "def extract_lora_weights(model):\n",
    "    \"\"\"Extract LoRA weights from model for comparison.\"\"\"\n",
    "    weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and param.requires_grad:\n",
    "            weights[name] = param.detach().cpu().numpy().copy()\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Select only the first 5 batches for initial comparison\n",
    "def get_limited_batches(dataloader, num_batches=5):\n",
    "    \"\"\"Get a limited number of batches from dataloader.\"\"\"\n",
    "    limited_batches = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            limited_batches.append(batch)\n",
    "        else:\n",
    "            break\n",
    "    return limited_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8c6da",
   "metadata": {},
   "source": [
    "## Experiment: Comparing fhe=\"disable\" vs fhe=\"execute\"\n",
    "\n",
    "Train two identical models for 5 steps using the quantized clear model (disable mode) and compare against FHE fine-tuning (execute mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6964b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:00:20,057 - INFO - === Starting new training session ===\n",
      "2025-06-18 16:00:20,060 - INFO - len(self.remote_names)=1 Remote Modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers detected in the model.\n"
     ]
    }
   ],
   "source": [
    "# Original model trainer for reference\n",
    "lora_trainer = LoraTrainer(\n",
    "    model=peft_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=causal_lm_loss,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    training_args=vars(training_args),\n",
    "    n_layers_to_skip_for_backprop=3,\n",
    "    eval_loader=eval_dl,\n",
    "    eval_metric_fn=None,\n",
    "    logging_steps=1,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    train_log_path=TRAIN_LOG_FILE,\n",
    ")\n",
    "\n",
    "# Clear the existing file for our experiments\n",
    "with open(EVAL_RESPONSES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Experiment: disable vs simulate ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bcf3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing first model (fhe=disable)...\n",
      "['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:00:21,979 - INFO - === Starting new training session ===\n",
      "2025-06-18 16:00:21,982 - INFO - len(self.remote_names)=1 Remote Modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers detected in the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ff0f5c4e864e3c90820accbd7b1729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compiling FHE layers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ -------------- Using GLWE backend for quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:00:25,255 - INFO - Compilation complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pre-training model...\n",
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: I have a problem with my math. I know that 7 is the answer, but I can't figure out what number it is. I am\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training perplexity: 116.03\n",
      "\n",
      "Training with fhe=disable for 5 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eebcf1eee44b8aa8a43f8b3eb55565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:00:40,605 - INFO - Step 1: loss=2.659109, avg_loss=2.659109\n",
      "2025-06-18 16:00:40,664 - INFO - Average gradient magnitude: 0.725166\n",
      "2025-06-18 16:00:41,218 - INFO - Step 2: loss=4.473459, avg_loss=3.566284\n",
      "2025-06-18 16:00:41,262 - INFO - Average gradient magnitude: 1.305497\n",
      "2025-06-18 16:00:41,756 - INFO - Step 3: loss=6.448898, avg_loss=4.527155\n",
      "2025-06-18 16:00:41,800 - INFO - Average gradient magnitude: 3.180925\n",
      "2025-06-18 16:00:42,290 - INFO - Step 4: loss=0.942487, avg_loss=3.630988\n",
      "2025-06-18 16:00:42,333 - INFO - Average gradient magnitude: 0.215597\n",
      "2025-06-18 16:00:42,814 - INFO - Step 5: loss=0.824344, avg_loss=3.069659\n",
      "2025-06-18 16:00:42,858 - INFO - Average gradient magnitude: 0.247480\n",
      "2025-06-18 16:00:42,920 - INFO - Epoch 1 completed. Avg Loss: 3.069659, FHE Mode: disable\n",
      "2025-06-18 16:00:42,920 - INFO - Training completed. Final Avg Loss: 3.069659, FHE Mode: disable\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing first model (fhe=disable)...\")\n",
    "model_disable = initialize_model()\n",
    "\n",
    "print(list(model_disable.state_dict().keys())[:3])\n",
    "\n",
    "initial_weights_disable = extract_lora_weights(model_disable)\n",
    "\n",
    "trainer_disable, train_dl = setup_trainer(model_disable)\n",
    "\n",
    "# Evaluate pre-training\n",
    "print(\"Evaluating pre-training model...\")\n",
    "model_disable.eval()\n",
    "pre_training_metrics = metric_fn(model_disable, eval_dl)\n",
    "print(f\"Pre-training perplexity: {pre_training_metrics['perplexity']:.2f}\")\n",
    "\n",
    "# Get limited batches for consistent training\n",
    "limited_batches = get_limited_batches(train_dl, 5)\n",
    "\n",
    "# Train with fhe=disable for 5 steps\n",
    "print(\"\\nTraining with fhe=disable for 5 steps...\")\n",
    "trainer_disable.train(limited_batches, fhe=\"disable\", device=DEVICE)\n",
    "losses_disable = trainer_disable.get_training_losses()\n",
    "\n",
    "# Store LoRA weights\n",
    "weights_disable = extract_lora_weights(model_disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd66e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing second model (fhe=execute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:00:45,208 - INFO - === Starting new training session ===\n",
      "2025-06-18 16:00:45,211 - INFO - len(self.remote_names)=1 Remote Modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers detected in the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6bd083dd8f459eab9a8fea860fa318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compiling FHE layers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ -------------- Using GLWE backend for quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:00:50,158 - INFO - Compilation complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying initial weights are identical...\n",
      "Initial weights match between models.\n",
      "\n",
      "Training with fhe=execute for 5 steps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608c99d022d45b58ec8f75a8531adc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef41eb0de4914fbfb175f729f7762b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FHE Modules Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:01:56,587 - INFO - Step 1: loss=2.659086, avg_loss=2.659086\n",
      "2025-06-18 16:01:56,632 - INFO - Average gradient magnitude: 0.726246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c6cab0323d442cbee5cebfb55d2a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FHE Modules Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:03:16,610 - INFO - Step 2: loss=4.473416, avg_loss=3.566251\n",
      "2025-06-18 16:03:16,655 - INFO - Average gradient magnitude: 1.306394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d33ae8c84948518a3198c9358c822c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FHE Modules Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:04:57,121 - INFO - Step 3: loss=6.453128, avg_loss=4.528543\n",
      "2025-06-18 16:04:57,166 - INFO - Average gradient magnitude: 3.171206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb113476b3d4fd6844367e53751d3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FHE Modules Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:06:30,119 - INFO - Step 4: loss=0.942440, avg_loss=3.632017\n",
      "2025-06-18 16:06:30,165 - INFO - Average gradient magnitude: 0.215929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ca1adeff834f9eb8ae534b23e84f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FHE Modules Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:08:06,708 - INFO - Step 5: loss=0.824260, avg_loss=3.070466\n",
      "2025-06-18 16:08:06,750 - INFO - Average gradient magnitude: 0.247497\n",
      "2025-06-18 16:08:06,811 - INFO - Epoch 1 completed. Avg Loss: 3.070466, FHE Mode: execute\n",
      "2025-06-18 16:08:06,812 - INFO - Training completed. Final Avg Loss: 3.070466, FHE Mode: execute\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing second model (fhe=execute)...\")\n",
    "model_execute = initialize_model()\n",
    "trainer_execute, _ = setup_trainer(model_execute)\n",
    "\n",
    "# Verify weights are the same before training\n",
    "print(\"Verifying initial weights are identical...\")\n",
    "initial_weights_execute = extract_lora_weights(model_execute)\n",
    "weights_match = True\n",
    "for name, weight in initial_weights_disable.items():\n",
    "    if not np.allclose(weight, initial_weights_execute[name]):\n",
    "        print(f\"Weight mismatch in {name}\")\n",
    "        weights_match = False\n",
    "if weights_match:\n",
    "    print(\"Initial weights match between models.\")\n",
    "\n",
    "# Train with fhe=execute for 5 steps\n",
    "print(\"\\nTraining with fhe=execute for 5 steps...\")\n",
    "trainer_execute.train(limited_batches, fhe=\"execute\", device=DEVICE)\n",
    "losses_execute = trainer_execute.get_training_losses()\n",
    "\n",
    "# Compare LoRA weights after training\n",
    "weights_execute = extract_lora_weights(model_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8490d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing first model (fhe=remote)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:11:39,930 - INFO - === Starting new training session ===\n",
      "2025-06-18 16:11:39,942 - INFO - len(self.remote_names)=1 Remote Modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers detected in the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289255e8df564bb7b5987feb53750714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compiling FHE layers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ -------------- Using GLWE backend for quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:11:44,418 - INFO - Compilation complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pre-training model...\n",
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: I have a problem with my math. I know that 7 is the answer, but I can't figure out what number it is. I am\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training perplexity: 116.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:12:08,533 - INFO - Model saved at compiled_models/meta-llama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [init_client] Generating keys...\n",
      "ğŸ“¡ [init_client] Adding keys: http://0.0.0.0:8000/add_key\n",
      "ğŸ“¡ [init_client] Key added with UID: 93ebbbdc-3630-4b36-913a-09844cf0433b\n",
      "\n",
      "Training with fhe=remote for 5 steps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcea73eaa7845199ce61f30a93d0c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 64, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:13:16,594 - INFO - Step 1: loss=2.686832, avg_loss=2.686832\n",
      "2025-06-18 16:13:16,638 - INFO - Average gradient magnitude: 0.632487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 64, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:14:24,558 - INFO - Step 2: loss=3.821261, avg_loss=3.254047\n",
      "2025-06-18 16:14:24,600 - INFO - Average gradient magnitude: 1.240014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 64, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:16:00,191 - INFO - Step 3: loss=5.611121, avg_loss=4.039738\n",
      "2025-06-18 16:16:00,237 - INFO - Average gradient magnitude: 2.226698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 64, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:17:45,904 - INFO - Step 4: loss=2.913603, avg_loss=3.758204\n",
      "2025-06-18 16:17:45,951 - INFO - Average gradient magnitude: 0.549885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 64, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:19:25,636 - INFO - Step 5: loss=3.301362, avg_loss=3.666836\n",
      "2025-06-18 16:19:25,681 - INFO - Average gradient magnitude: 0.535608\n",
      "2025-06-18 16:19:25,742 - INFO - Epoch 1 completed. Avg Loss: 3.666836, FHE Mode: remote\n",
      "2025-06-18 16:19:25,742 - INFO - Training completed. Final Avg Loss: 3.666836, FHE Mode: remote\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing first model (fhe=remote)...\")\n",
    "model_remote = initialize_model()\n",
    "\n",
    "initial_weights_disable = extract_lora_weights(model_remote)\n",
    "\n",
    "trainer_remote, train_dl = setup_trainer(model_remote)\n",
    "\n",
    "# Evaluate pre-training\n",
    "print(\"Evaluating pre-training model...\")\n",
    "model_remote.eval()\n",
    "pre_training_metrics = metric_fn(model_remote, eval_dl)\n",
    "print(f\"Pre-training perplexity: {pre_training_metrics['perplexity']:.2f}\")\n",
    "\n",
    "\n",
    "MODEL_DIR = Path('compiled_models') / 'meta-llama'\n",
    "\n",
    "trainer_remote.save_and_clear_private_info(MODEL_DIR, via_mlir=True)\n",
    "\n",
    "PATH_TO_CLIENTS = Path('compiled_models') / 'meta-llama'\n",
    "PATH_TO_CLIENTS_KEYS = Path('compiled_models') / f\"meta-llama_keys\"\n",
    "\n",
    "trainer_remote.hybrid_model.init_client(\n",
    "    path_to_clients=PATH_TO_CLIENTS,\n",
    "    path_to_keys=PATH_TO_CLIENTS_KEYS\n",
    ")\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEMode\n",
    "\n",
    "trainer_remote.hybrid_model.set_fhe_mode(HybridFHEMode.REMOTE)\n",
    "\n",
    "\n",
    "# Get limited batches for consistent training\n",
    "limited_batches = get_limited_batches(train_dl, 5)\n",
    "\n",
    "# Train with fhe=disable for 5 steps\n",
    "print(\"\\nTraining with fhe=remote for 5 steps...\")\n",
    "trainer_remote.train(limited_batches, fhe=\"remote\", device=DEVICE)\n",
    "losses_remote = trainer_remote.get_training_losses()\n",
    "\n",
    "# Store LoRA weights\n",
    "weights_remote = extract_lora_weights(model_remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef20b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "steps = np.arange(len(losses_disable))\n",
    "\n",
    "plt.plot(steps, losses_disable, label=\"Disable Loss\", color=\"blue\")\n",
    "plt.plot(steps, losses_execute, label=\"Execute Loss\", color=\"red\", linestyle=\"--\")\n",
    "plt.plot(steps, losses_remote, label=\"Remote Loss\", color=\"green\", linestyle=\"--\")\n",
    "\n",
    "plt.scatter(steps, losses_disable, color=\"blue\", s=100)\n",
    "plt.scatter(steps, losses_execute, color=\"red\", marker=\"s\", s=100)\n",
    "plt.scatter(steps, losses_remote, color=\"green\", marker=\"x\", s=100)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(steps)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c94d51b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 20, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(20, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(20, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([20, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 20, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 20, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([1, 1, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(1, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(1, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([1, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 1, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 1, 2048])\n",
      "ğŸ -------------- len(inferences)=1, type(inferences)=<class 'list'>\n",
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: 8. What is that number 7. 98. What is that number 7. What is that number 7. What is that number\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> remote_glwe_call\n",
      "ğŸ -------------- Batch size: torch.Size([4, 64, 2048]) <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Sample_0:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "\n",
      "\n",
      "Sample_1:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "\n",
      "\n",
      "Sample_2:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n",
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "\n",
      "\n",
      "Sample_3:\n",
      "\n",
      "ğŸ -------------- q_x_sample.shape=(64, 2048)\n",
      "ğŸ -------------- self.uid='93ebbbdc-3630-4b36-913a-09844cf0433b'\n",
      "ğŸ -------------- self.private_remote_weights_path=PosixPath('/Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server')\n",
      "âœ…âœ…âœ… Data Sent\n",
      "Starting inference ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…âœ…âœ… FHE successully computed\n",
      "ğŸ“¥ Encrypted bundle saved at /Users/kcelia/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/compiled_models/meta-llama/inference_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer/po_1_64_2048_pc/server/encrypted_output_from_server.bin\n",
      "ğŸ -------------- weight_shape=(2048, 2048)\n",
      "ğŸ -------------- weight_scale=tensor([[0.0029, 0.0027, 0.0036,  ..., 0.0026, 0.0030, 0.0026]])\n",
      "ğŸ -------------- weight_zp=tensor([[52., 73., 67.,  ..., 56., 72., 62.]])\n",
      "ğŸ -------------- sum_w=tensor([105733., 150504., 137896.,  ..., 114373., 147414., 127626.])\n",
      "ğŸ -------------- type(bias)=<class 'NoneType'>\n",
      "num_valid_glwe_values_in_last_ciphertext=2048 - self.executor.poly_size=2048\n",
      "âœ…âœ…âœ… Output decrypted\n",
      "ğŸ -------------- q_result.shape=(64, 2048), type(q_result)=<class 'numpy.ndarray'>\n",
      "ğŸ -------------- result_tensor.shape=torch.Size([64, 2048]), type(result_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Output Dequantized\n",
      "ğŸ -------------- out_tensor.shape=torch.Size([1, 64, 2048]), type(out_tensor)=<class 'torch.Tensor'>\n",
      "âœ…âœ…âœ… Bias added\n",
      "ğŸ -------------- torch.Size([1, 64, 2048])\n",
      "ğŸ -------------- len(inferences)=4, type(inferences)=<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 64, 32, 64]' is invalid for input of size 2097152",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model_remote.eval()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m metrics_final = \u001b[43mmetric_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_remote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal perplexity after extended training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_final[\u001b[33m'\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmetric_fn\u001b[39m\u001b[34m(model, dataloader)\u001b[39m\n\u001b[32m     23\u001b[39m batch_labels = batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].to(DEVICE)\n\u001b[32m     24\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m.logits\n\u001b[32m     26\u001b[39m valid = batch_labels[..., \u001b[32m1\u001b[39m:] != -\u001b[32m100\u001b[39m\n\u001b[32m     27\u001b[39m loss = F.cross_entropy(\n\u001b[32m     28\u001b[39m     outputs[..., :-\u001b[32m1\u001b[39m, :].contiguous().view(-\u001b[32m1\u001b[39m, outputs.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m     29\u001b[39m     batch_labels[..., \u001b[32m1\u001b[39m:].contiguous().view(-\u001b[32m1\u001b[39m),\n\u001b[32m     30\u001b[39m     ignore_index=-\u001b[32m100\u001b[39m,\n\u001b[32m     31\u001b[39m     reduction=\u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/peft/peft_model.py:1577\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1575\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1576\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1577\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1578\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1579\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1580\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1581\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1588\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1590\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:188\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[39m\n\u001b[32m   1187\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1189\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1203\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pretraining_tp > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    933\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    934\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    935\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    942\u001b[39m         position_embeddings,\n\u001b[32m    943\u001b[39m     )\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    673\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    689\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zama/concrete-ml/use_case_examples/llama_finetuning_with_salad/.lora/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:563\u001b[39m, in \u001b[36mLlamaSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    560\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states)\n\u001b[32m    561\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m query_states = \u001b[43mquery_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    564\u001b[39m key_states = key_states.view(bsz, q_len, \u001b[38;5;28mself\u001b[39m.num_key_value_heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    565\u001b[39m value_states = value_states.view(bsz, q_len, \u001b[38;5;28mself\u001b[39m.num_key_value_heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[4, 64, 32, 64]' is invalid for input of size 2097152"
     ]
    }
   ],
   "source": [
    "model_remote.eval()\n",
    "metrics_final = metric_fn(model_remote, eval_dl)\n",
    "print(f\"Final perplexity after extended training: {metrics_final['perplexity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b91fdf",
   "metadata": {},
   "source": [
    "## Full Training\n",
    "\n",
    "After validating the model's FHE behavior, the quantized model is trained in clear on the filtered dataset to validate convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:44:55,076 - INFO - === Starting new training session ===\n",
      "2025-06-18 15:44:55,080 - INFO - len(self.remote_names)=1 Remote Modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers detected in the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cde944589554c3294608cdd91223025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compiling FHE layers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ -------------- Using GLWE backend for quantization\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:44:59,621 - INFO - Compilation complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edb1c615c1743959254038c0af16602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:45:59,109 - INFO - Step 100: loss=0.309508, avg_loss=0.621845\n",
      "2025-06-18 15:45:59,154 - INFO - Average gradient magnitude: 0.200717\n",
      "2025-06-18 15:45:59,155 - INFO - Running evaluation at step 100...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: The number you multiply by 7 is 98.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:46:14,290 - INFO - [Evaluation at step 100] perplexity: 1.3884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:47:12,343 - INFO - Step 200: loss=0.230378, avg_loss=0.465844\n",
      "2025-06-18 15:47:12,387 - INFO - Average gradient magnitude: 0.310229\n",
      "2025-06-18 15:47:12,388 - INFO - Running evaluation at step 200...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: The number that 7 times 98 is 98 is 7 * 98 = 686.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:47:28,767 - INFO - [Evaluation at step 200] perplexity: 1.4474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:48:25,830 - INFO - Step 300: loss=0.103340, avg_loss=0.454782\n",
      "2025-06-18 15:48:25,875 - INFO - Average gradient magnitude: 0.105821\n",
      "2025-06-18 15:48:25,875 - INFO - Running evaluation at step 300...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: If you multiply 7 by 98, you get 686.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:48:40,998 - INFO - [Evaluation at step 300] perplexity: 1.3874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:49:39,188 - INFO - Step 400: loss=0.073380, avg_loss=0.430782\n",
      "2025-06-18 15:49:39,233 - INFO - Average gradient magnitude: 0.089474\n",
      "2025-06-18 15:49:39,234 - INFO - Running evaluation at step 400...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: When you multiply a number by 7, it becomes 98.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:49:54,536 - INFO - [Evaluation at step 400] perplexity: 1.3708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:50:54,047 - INFO - Step 500: loss=0.266026, avg_loss=0.411111\n",
      "2025-06-18 15:50:54,092 - INFO - Average gradient magnitude: 0.256978\n",
      "2025-06-18 15:50:54,092 - INFO - Running evaluation at step 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: If you multiply a number by 7, it becomes 98. \n",
      "\n",
      "So, the number that you multiply by 7 is 98.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:51:11,080 - INFO - [Evaluation at step 500] perplexity: 1.3251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:52:10,639 - INFO - Step 600: loss=0.157061, avg_loss=0.393064\n",
      "2025-06-18 15:52:10,686 - INFO - Average gradient magnitude: 0.199021\n",
      "2025-06-18 15:52:10,686 - INFO - Running evaluation at step 600...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: The number that becomes 98 when you multiply 7 by it is 14.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:52:27,383 - INFO - [Evaluation at step 600] perplexity: 1.3226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:53:27,877 - INFO - Step 700: loss=0.012618, avg_loss=0.375134\n",
      "2025-06-18 15:53:27,929 - INFO - Average gradient magnitude: 0.035549\n",
      "2025-06-18 15:53:27,929 - INFO - Running evaluation at step 700...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: To find the number, you need to divide 98 by 7. \n",
      "\n",
      "98 Ã· 7 = 14.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:53:44,080 - INFO - [Evaluation at step 700] perplexity: 1.3237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:54:45,801 - INFO - Step 800: loss=0.177659, avg_loss=0.367969\n",
      "2025-06-18 15:54:45,847 - INFO - Average gradient magnitude: 0.307541\n",
      "2025-06-18 15:54:45,847 - INFO - Running evaluation at step 800...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: To find the number, you need to divide 98 by 7. \n",
      "\n",
      "98 Ã· 7 = 14\n",
      "\n",
      "So the number is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:55:02,813 - INFO - [Evaluation at step 800] perplexity: 1.2904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:56:03,237 - INFO - Step 900: loss=0.163814, avg_loss=0.355055\n",
      "2025-06-18 15:56:03,283 - INFO - Average gradient magnitude: 0.078287\n",
      "2025-06-18 15:56:03,283 - INFO - Running evaluation at step 900...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When you multiply a number by 7, it becomes 98. What is that number?\n",
      "\n",
      "Response: To find the number, you need to divide 98 by 7. \n",
      "\n",
      "98 Ã· 7 = 14\n",
      "\n",
      "So, the number is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:56:20,656 - INFO - [Evaluation at step 900] perplexity: 1.2585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n",
      "\n",
      "ğŸ“¡ [HybridFHEModel] Keys: self.executor.private_key is not None=False\n"
     ]
    }
   ],
   "source": [
    "# Fully train the model in disable mode\n",
    "model_final = initialize_model()\n",
    "trainer_final, full_train_dl = setup_trainer(model_final, logging_steps=100)\n",
    "\n",
    "trainer_final.train(full_train_dl, fhe=\"disable\", device=DEVICE)\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nEvaluating final model after full training...\")\n",
    "model_final.eval()\n",
    "metrics_final = metric_fn(model_final, eval_dl)\n",
    "print(f\"Final perplexity after extended training: {metrics_final['perplexity']:.2f}\")\n",
    "print(\n",
    "    f\"Improvement from initial model: \"\n",
    "    f\"{pre_training_metrics['perplexity'] - metrics_final['perplexity']:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a111c",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the fine-tuned model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PATH.is_dir() and any(SAVE_PATH.iterdir()):\n",
    "    shutil.rmtree(SAVE_PATH)\n",
    "trainer_final.save_and_clear_private_info(SAVE_PATH)\n",
    "print(\"Model saved to:\", SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": ".lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
