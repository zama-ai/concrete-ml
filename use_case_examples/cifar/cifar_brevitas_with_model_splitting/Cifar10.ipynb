{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442b1f40-24b9-4651-83f0-dee2f042a17a",
   "metadata": {},
   "source": [
    "# CIFAR-10 FHE classification with 8-bit split VGG\n",
    "\n",
    "As mentioned in the [README](./README.md) we present in this notebook how to compile to FHE a split torch model.\n",
    "The model we will be considering is a CIFAR-10 classifier based on the VGG architecture. It was trained with pruning and accumulator bit-width monitoring so that the classifier does not exceed the 8 bit-width accumulator constraint.\n",
    "\n",
    "The first layers of the models should be run on the clear data on the client's side and the rest of the model in FHE on the server's side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423cb30c-febf-4b5a-b5ce-505ac632b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from model import CNV  # pylint: disable=no-name-in-module\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torchvision import transforms\n",
    "\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff5c60-669a-4dcc-a0e2-fd17d7e2cdd7",
   "metadata": {},
   "source": [
    "In `model.py` we define our model architecture.\n",
    "\n",
    "As one can see we split the main model `CNV` into two sub-models `ClearModule` and `EncryptedModule`.\n",
    "\n",
    "- `ClearModule` will be used to run on clear data on the client's side. It can do any float operations and does not require quantization.\n",
    "- `EncryptedModule` will run on the server side. This part of the model running in FHE we need to quantize it, thus why we leverage Brevitas for Quantization Aware Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bb6a59-1002-4744-bfe7-baef97a4c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNV(num_classes=10, weight_bit_width=2, act_bit_width=2, in_bit_width=3, in_ch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea920f40-82e1-4c00-8c65-035de4b11d78",
   "metadata": {},
   "source": [
    "We won't be training the model is this notebook as it would be quite computationnaly intensive but we provide an already trained model that satisfies the 8-bit accumulator size constraint and that performs better than random on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cd2622-b0f4-4632-8ff7-8d17a82ac812",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load(\"./8_bit_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476ddd8e-bb04-4d42-844c-cde14b5817e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(loaded[\"model_state_dict\"])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e7ce1e-c287-48e5-ae6d-29df15a12110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to .data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c15f224f214f9798a29c77b0433e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .data/cifar-10-python.tar.gz to .data/\n",
      "(Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: .data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           ), Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: .data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           ))\n"
     ]
    }
   ],
   "source": [
    "IMAGE_TRANSFORM = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=\".data/\",\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=IMAGE_TRANSFORM,\n",
    "        target_transform=None,\n",
    "    )\n",
    "except RuntimeError:\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=\".data/\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=IMAGE_TRANSFORM,\n",
    "        target_transform=None,\n",
    "    )\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root=\".data/\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=IMAGE_TRANSFORM,\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "print((train_set, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c9182-ae7b-47f3-b810-d623bddad1dc",
   "metadata": {},
   "source": [
    "We use a sub-sample of the training set for the FHE compilation to maintain acceptable compilation times and avoid out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9de82bb-a05d-4b34-b6ed-15ffe75014ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "train_sub_set = torch.stack(\n",
    "    [train_set[index][0] for index in range(min(num_samples, len(train_set)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841a675-185c-4723-9c8f-a1bcc9e17cf2",
   "metadata": {},
   "source": [
    "Since we will be compiling only a part of the network we need to give it representative inputs, in our case the first feature map of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ed8dfb-5f5c-4dad-ba2b-6bc74dfccc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing -> images -> feature maps\n",
    "with torch.no_grad():\n",
    "    train_features_sub_set = model.clear_module(train_sub_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93f390-88d6-41a5-acf4-67dc42b80fe4",
   "metadata": {},
   "source": [
    "# FHE Simulation\n",
    "\n",
    "In a first time we can make sure that our FHE constraints are respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2af71c-baba-4edb-8cc3-f3795ae31313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model\n",
      "Compilation finished in 93.09 seconds\n",
      "Max bitwidth: 8 bits!\n"
     ]
    }
   ],
   "source": [
    "optional_kwargs = {}\n",
    "\n",
    "# Compile the model\n",
    "compilation_onnx_path = \"compilation_model.onnx\"\n",
    "print(\"Compiling the model\")\n",
    "start_compile = time.time()\n",
    "quantized_numpy_module = compile_brevitas_qat_model(\n",
    "    # our encrypted model\n",
    "    torch_model=model.encrypted_module,\n",
    "    # a representative input-set to be used for compilation\n",
    "    torch_inputset=train_features_sub_set,\n",
    "    **optional_kwargs,\n",
    "    output_onnx_file=compilation_onnx_path,\n",
    ")\n",
    "\n",
    "end_compile = time.time()\n",
    "print(f\"Compilation finished in {end_compile - start_compile:.2f} seconds\")\n",
    "\n",
    "# Check that the network is compatible with FHE constraints\n",
    "assert quantized_numpy_module.fhe_circuit is not None\n",
    "bitwidth = quantized_numpy_module.fhe_circuit.graph.maximum_integer_bit_width()\n",
    "print(f\"Max bit-width: {bitwidth} bits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114f4fad-9f9a-4006-9337-6a0ee7a1c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = train_set[0]\n",
    "with torch.no_grad():\n",
    "    feature_maps = model.clear_module(img[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1e3763-9925-4537-94e1-bb6d32bd41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_simulated = quantized_numpy_module.forward(feature_maps.numpy(), fhe=\"simulate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db4f92d-6092-418f-b5e1-0aba544ffa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch_output = model(img[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c3aa51-fb4d-4da3-9564-6be636b42799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0171,  0.0171, -0.0215,  0.0122,  0.0232, -0.0144,  0.0042, -0.0115,\n",
      "          0.0180,  0.0065]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch_output - output_simulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398cdb2-b9aa-4937-97c2-0e5c41d1ac00",
   "metadata": {},
   "source": [
    "We see that we have some differences between the output of the torch model output and the FHE simulation.\n",
    "\n",
    "This is expected but as we can see in the following code blocks we have no difference in top-k accuracies between Pytorch and the FHE simulation mode.\n",
    "\n",
    "It appears that there are some differences between the output of the Torch model and the FHE simulation. While this outcome was expected, it is important to note that, as demonstrated in the following code blocks, there are no differences in the top-k accuracies between PyTorch and the FHE simulation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c34ffdb-6aca-4464-b26d-bb12c4afb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(file_path: str, k=3):\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    prob_columns = [elt for elt in predictions.columns if elt.endswith(\"_prob\")]\n",
    "    predictions[\"pred_label\"] = predictions[prob_columns].values.argmax(axis=1)\n",
    "\n",
    "    # Equivalent to top-1-accuracy\n",
    "    for k_ in range(1, k + 1):\n",
    "        print(\n",
    "            f\"top-{k}-accuracy: \",\n",
    "            top_k_accuracy_score(\n",
    "                y_true=predictions[\"label\"], y_score=predictions[prob_columns], k=k_\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732267b9-c7cb-42ee-a40a-b21337d41430",
   "metadata": {},
   "source": [
    "We can use the `infer_fhe_simulation.py` script to generate the predictions of the model using Pytorch for the first layer and FHE simulation for the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5918f4d-91dc-434a-90c8-baf79e13f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model\n",
      "Compilation finished in 86.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inference\n",
      "top-3-accuracy:  0.6231\n",
      "top-3-accuracy:  0.8072\n",
      "top-3-accuracy:  0.8906\n"
     ]
    }
   ],
   "source": [
    "%run infer_fhe_simulation.py\n",
    "evaluate(\"./fhe_simulated_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee3362-62c0-4e4a-a4c2-4af63710e7dc",
   "metadata": {},
   "source": [
    "And the `infer.py` script to generate the pure Pytorch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65849856-cf80-4b93-99c1-56deeac83c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inference\n",
      "top-3-accuracy:  0.6231\n",
      "top-3-accuracy:  0.8072\n",
      "top-3-accuracy:  0.8906\n"
     ]
    }
   ],
   "source": [
    "%run infer_torch.py\n",
    "evaluate(\"./predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a2eb9-7e4f-4890-b8c5-71765bb8bb18",
   "metadata": {},
   "source": [
    "# FHE execution results\n",
    "\n",
    "In this notebook we showed how to compile a split-VGG model trained to classify CIFAR-10 images in FHE.\n",
    "\n",
    "While satisfying the FHE constraints the model achieves the following performances:\n",
    "\n",
    "- top-1-accuracy: 0.6234\n",
    "- top-2-accuracy: 0.8075\n",
    "- top-3-accuracy: 0.8905\n",
    "\n",
    "*We don't launch the inference in FHE in this notebook as it takes quite some time just to infer on one image.*\n",
    "\n",
    "For reference we ran the inference of one image on an AWS c6i.metal compute machine, using the `fhe_inference.py` script, and got the following timings:\n",
    "\n",
    "- Time to compile: 103 seconds\n",
    "- Time to keygen: 639 seconds\n",
    "- Time to infer: ~1800 seconds"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
