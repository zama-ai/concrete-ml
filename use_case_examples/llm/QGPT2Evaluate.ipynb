{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of GPT-2 models with FHE-compliant operators \n",
    "\n",
    "This notebook presents a first approach of how to execute a GPT model in FHE, where some specific parts of the model are converted to FHE computations.\n",
    "In the following, we consider the GPT-2 model with the language modeling head on top, with the following configuration: 12 layers, 12 attention heads, 768 embedding dimensions and a vocabulary size of 50257 words.\n",
    "Additionally, our QGPT-2 models are built around Hugging Face's Transformer library, sharing the same API with only a few additional steps to acknowledge.\n",
    "\n",
    "We therefore evaluate the performance of two quantized versions of the GPT-2 model:\n",
    "- a model that quantizes a single attention head found in the first layer: SingleHeadQGPT2Model\n",
    "- a model that quantizes a complete multi-head attention pass found in the first layer: MultiHeadsQGPT2Model\n",
    "\n",
    "The quantized operators from these models are FHE-compliant, meaning that these specific part can be executed in FHE, while the rest of the model are done in float in the clear.\n",
    "We therefore explain how to load these models from the Hugging Face's associated pre-trained ones, calibrate them, compile their FHE circuit and then execute their inference with some FHE.\n",
    "Finally, we compare different top-k accuracies on the next token predicted from a base text for both models with respect to the number of bits of quantization used. Using these figures, we show that inputs and weights can be quantized over less than 8 bits to make the inference reach near-floating point performances.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from load_huggingface import get_gpt2_model, get_gpt2_tokenizer\n",
    "from qgpt2_models import MultiHeadsQGPT2Model, SingleHeadQGPT2Model\n",
    "\n",
    "# Disable some warnings from Hugging Face when loading the models\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the GPT-2 model (GPT2LMHeadModel) and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = get_gpt2_model(\"gpt2_model\")\n",
    "gpt2_tokenizer = get_gpt2_tokenizer(\"gpt2_tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Hugging Face\n",
    "\n",
    "First, we show a simple example using a short sequence and generate a few tokens with Hugging Face's GPT-2 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Computations on encrypted data can help\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now encode the sequence using the tokenizer and retrieve the input token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_indexes = gpt2_tokenizer.encode(input_sentence)\n",
    "input_ids = torch.tensor(input_token_indexes).unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate 10 new tokens and decode the output sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Computations on encrypted data can help protect your privacy.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = gpt2_model.generate(input_ids, max_new_tokens=4, use_cache=False)\n",
    "gpt2_tokenizer.decode(output_ids[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Attention Head Model\n",
    "\n",
    "Here, the model's first attention head of its first layer, as well as its associated first projection, is done using quantized operators. \n",
    "The rest remains the same as in Hugging Face's implementation. Mode details can be found in SingleHeadQGPT2Model's documentation.\n",
    "\n",
    "We first load the model using the pre-trained files. \n",
    "Only 7 bits of quantization is needed in order to recover the same sentence as with the initial floating point model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_single_head_qgpt2 = SingleHeadQGPT2Model.from_pretrained(\n",
    "    \"gpt2_model\", n_bits=7, use_cache=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate the next 10 tokens in the clear with the quantized operators in order to retrieve the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Computations on encrypted data can help protect your privacy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_single_head_qgpt2.set_fhe_mode(fhe=\"disable\")\n",
    "\n",
    "output_ids_clear = proj_single_head_qgpt2.generate(input_ids, max_new_tokens=4)\n",
    "gpt2_tokenizer.decode(output_ids_clear[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sentence exactly matches the one from Hugging Face original model.\n",
    "\n",
    "We now generate the logits for the next predicted token in the clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_single_head_qgpt2.set_fhe_mode(fhe=\"disable\")\n",
    "\n",
    "output_logits_clear = proj_single_head_qgpt2(input_ids).logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compile the model using the input token ids. \n",
    "Here, the model executes the forward pass in the clear, which computes and stores the necessary quantization parameters. \n",
    "Once this is done, a FHE circuit is built, which then can be used to execute the forward pass with its quantized parts done in FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit compiled with at most 15 bits\n"
     ]
    }
   ],
   "source": [
    "circuit_single_head = proj_single_head_qgpt2.compile(input_ids, msbs_round=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the circuit's bit-width reaches 15 bits at most.\n",
    "\n",
    "Now let's set the model in simulation mode in order to retrieve the logits that are expected to be computed with FHE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_single_head_qgpt2.set_fhe_mode(fhe=\"simulate\")\n",
    "\n",
    "output_logits_simulated = proj_single_head_qgpt2(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated logits are equal to the quantized clear ones: False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Simulated logits are equal to the quantized clear ones:\",\n",
    "    torch.equal(output_logits_clear, output_logits_simulated),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Model\n",
    "\n",
    "Here, the model's multi-head attention found in its first layer is done using quantized operators. \n",
    "The rest remains the same as in Hugging Face's implementation. Mode details can be found in MultiHeadsQGPT2Model's documentation.\n",
    "\n",
    "We first load the model using the pre-trained files. \n",
    "In this case, 10 bits of quantization is needed in order to recover almost the same sentence as with the initial floating point model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_12_heads_qgpt2 = MultiHeadsQGPT2Model.from_pretrained(\"gpt2_model\", n_bits=10, use_cache=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we generate new tokens in the clear and decode the output sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Computations on encrypted data can help protect your privacy.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_12_heads_qgpt2.set_fhe_mode(fhe=\"disable\")\n",
    "\n",
    "output_ids_clear = proj_12_heads_qgpt2.generate(input_ids, max_new_tokens=4)\n",
    "gpt2_tokenizer.decode(output_ids_clear[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sentence matches the one from Hugging Face original model. However, in its current form, the MHA part need less precision to run on encrypted data. We can reduce the precision to 7 bits for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Computations on encrypted data can help protect your data from'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_12_heads_qgpt2 = MultiHeadsQGPT2Model.from_pretrained(\"gpt2_model\", n_bits=7, use_cache=False)\n",
    "proj_12_heads_qgpt2.set_fhe_mode(fhe=\"disable\")\n",
    "\n",
    "output_ids_clear = proj_12_heads_qgpt2.generate(input_ids, max_new_tokens=4)\n",
    "gpt2_tokenizer.decode(output_ids_clear[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the meaning is pretty close and now we can check the compilation to FHE.\n",
    "\n",
    "We now generate the logits for the next predicted token in the clear and then compile the model the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_12_heads_qgpt2.set_fhe_mode(fhe=\"disable\")\n",
    "\n",
    "output_logits_clear = proj_12_heads_qgpt2(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit compiled with at most 15 bits\n"
     ]
    }
   ],
   "source": [
    "circuit_12_heads = proj_12_heads_qgpt2.compile(input_ids, msbs_round=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the circuit reaches 15 bits of precision.\n",
    "\n",
    "We then execute it with simulation and observe that we once again retrieve the same outputs as the ones computes in the clear with quantized operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_12_heads_qgpt2.set_fhe_mode(fhe=\"simulate\")\n",
    "\n",
    "output_logits_simulated = proj_12_heads_qgpt2(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated logits are equal to the quantized clear ones: False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Simulated logits are equal to the quantized clear ones:\",\n",
    "    torch.equal(output_logits_clear, output_logits_simulated),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "In the following, we evaluate the impact of the number of bits used for quantization on the models' performance.\n",
    "Here, we check for the top-k accuracies, with a few different values of k, on the predicted next logits with respected to the one computed by the initial floating point model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SingleHeadQGPT2Model, n_bits: 2, Average Top-1 Accuracy: 0.7217\n",
      "Model: SingleHeadQGPT2Model, n_bits: 3, Average Top-1 Accuracy: 0.7217\n",
      "Model: SingleHeadQGPT2Model, n_bits: 4, Average Top-1 Accuracy: 0.9586\n",
      "Model: SingleHeadQGPT2Model, n_bits: 5, Average Top-1 Accuracy: 0.9561\n",
      "Model: SingleHeadQGPT2Model, n_bits: 6, Average Top-1 Accuracy: 0.9783\n",
      "Model: SingleHeadQGPT2Model, n_bits: 7, Average Top-1 Accuracy: 0.9854\n",
      "Model: SingleHeadQGPT2Model, n_bits: 8, Average Top-1 Accuracy: 0.9949\n",
      "Model: SingleHeadQGPT2Model, n_bits: 9, Average Top-1 Accuracy: 0.9981\n",
      "Model: SingleHeadQGPT2Model, n_bits: 10, Average Top-1 Accuracy: 0.9968\n",
      "Model: SingleHeadQGPT2Model, n_bits: 11, Average Top-1 Accuracy: 0.9975\n",
      "Model: SingleHeadQGPT2Model, n_bits: 2, Average Top-5 Accuracy: 0.9153\n",
      "Model: SingleHeadQGPT2Model, n_bits: 3, Average Top-5 Accuracy: 0.9153\n",
      "Model: SingleHeadQGPT2Model, n_bits: 4, Average Top-5 Accuracy: 0.9987\n",
      "Model: SingleHeadQGPT2Model, n_bits: 5, Average Top-5 Accuracy: 0.9994\n",
      "Model: SingleHeadQGPT2Model, n_bits: 6, Average Top-5 Accuracy: 1.0000\n",
      "Model: SingleHeadQGPT2Model, n_bits: 7, Average Top-5 Accuracy: 1.0000\n",
      "Model: SingleHeadQGPT2Model, n_bits: 8, Average Top-5 Accuracy: 1.0000\n",
      "Model: SingleHeadQGPT2Model, n_bits: 9, Average Top-5 Accuracy: 1.0000\n",
      "Model: SingleHeadQGPT2Model, n_bits: 10, Average Top-5 Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def generate_topk_tokens(model, tokenizer, text, top_k):\n",
    "    \"\"\"Generate the top-k tokens for every token in the text.\"\"\"\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    topk_tokens_list = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        for logit in logits[0]:\n",
    "            topk_tokens = torch.topk(\n",
    "                logit,\n",
    "                top_k,\n",
    "            ).indices.tolist()\n",
    "            topk_tokens_list.append(topk_tokens)\n",
    "    return topk_tokens_list\n",
    "\n",
    "\n",
    "def calculate_topk_accuracy(ground_truth_tokens, computed_tokens):\n",
    "    \"\"\"Calculate the top-k accuracy.\"\"\"\n",
    "    correct_count = sum([1 for gt_token in ground_truth_tokens if gt_token in computed_tokens])\n",
    "    return correct_count / len(ground_truth_tokens)\n",
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = {\n",
    "    \"SingleHeadQGPT2Model\": SingleHeadQGPT2Model,\n",
    "    \"MultiHeadsQGPT2Model\": MultiHeadsQGPT2Model,\n",
    "}\n",
    "\n",
    "# Define the different number of bits and k values to evaluate\n",
    "n_bits_range = list(range(2, 12))\n",
    "top_ks = [1, 5, 10]\n",
    "\n",
    "# Define the text data to use for the evaluation\n",
    "dataset_file = \"data.txt\"\n",
    "\n",
    "# Read the evaluation file\n",
    "with open(dataset_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    input_texts = [line.strip() for line in file]\n",
    "\n",
    "for model_name, Model in models.items():\n",
    "    plt.figure()\n",
    "    for top_k in top_ks:\n",
    "        accuracies = []\n",
    "        for n_bits in n_bits_range:\n",
    "\n",
    "            # Load the model using the current number of bits and set it to clear quantized mode\n",
    "            model = Model.from_pretrained(\n",
    "                \"gpt2_model\",\n",
    "                n_bits=n_bits,\n",
    "            )\n",
    "            model.set_fhe_mode(fhe=\"disable\", true_float=False)\n",
    "\n",
    "            accuracy_scores = []\n",
    "            for text in input_texts:\n",
    "                # Generate the top-k tokens for the Hugging Face floating point model\n",
    "                hf_topk_tokens_list = generate_topk_tokens(gpt2_model, gpt2_tokenizer, text, 1)\n",
    "\n",
    "                # Generate the top-k tokens for the clone model\n",
    "                clone_topk_tokens_list = generate_topk_tokens(model, gpt2_tokenizer, text, top_k)\n",
    "\n",
    "                # Compute the top-k accuracy for each token in the text\n",
    "                for hf_topk_tokens, clone_topk_tokens in zip(\n",
    "                    hf_topk_tokens_list, clone_topk_tokens_list\n",
    "                ):\n",
    "                    topk_accuracy = calculate_topk_accuracy(hf_topk_tokens, clone_topk_tokens)\n",
    "                    accuracy_scores.append(topk_accuracy)\n",
    "\n",
    "            # Compute the average accuracy within the text\n",
    "            average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "            accuracies.append(average_accuracy)\n",
    "\n",
    "            # Print the average accuracy for this model, n_bits, and top_k\n",
    "            print(\n",
    "                f\"Model: {model_name}, \"\n",
    "                f\"n_bits: {n_bits}, \"\n",
    "                f\"Average Top-{top_k} Accuracy: {average_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        plt.plot(n_bits_range, accuracies, label=f\"Top-{top_k}\")\n",
    "\n",
    "    # Plot the model's evaluation\n",
    "    plt.title(f\"{model_name}\")\n",
    "    plt.xlabel(\"n_bits\")\n",
    "    plt.ylabel(\"Average Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the first model, where only a single head if done with FHE-compliant operators, 4 bits are enough to recover 95\\% of Hugging Face's performance in terms of top1 accuracy. \n",
    "For the second model, which implements a complete multi-head attention with quantized operators, 7 bits gives a 80\\% exact predictions while the top-5 and top-10 accuracies reach 98\\%."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
