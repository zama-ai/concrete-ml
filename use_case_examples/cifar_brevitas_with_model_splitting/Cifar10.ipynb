{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442b1f40-24b9-4651-83f0-dee2f042a17a",
   "metadata": {},
   "source": [
    "# CIFAR-10 FHE classification with 8-bit split VGG\n",
    "\n",
    "As mentionned in the [README](./README.md) we present in this notebook how to compile to FHE a splitted torch model.\n",
    "The model we will be considering is a CIFAR-10 classifier based on the VGG architecture. It was trained with pruning and accumulator bit-width monitoring so that the classifier does not exceed the 8 bit-width accumulator constraint.\n",
    "\n",
    "The first layers of the models should be run on the clear data on the client's side and the rest of the model in FHE on the server's side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423cb30c-febf-4b5a-b5ce-505ac632b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from concrete.numpy import Configuration\n",
    "from IPython.display import clear_output\n",
    "from model import CNV  # pylint: disable=no-name-in-module\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torchvision import transforms\n",
    "\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff5c60-669a-4dcc-a0e2-fd17d7e2cdd7",
   "metadata": {},
   "source": [
    "In `model.py` we define our model architecture.\n",
    "\n",
    "As one can see we split the main model `CNV` into two sub-models `ClearModule` and `EncryptedModule`.\n",
    "\n",
    "- `ClearModule` will be used to run on clear data on the client's side. It can do any float operations and does not require quantization.\n",
    "- `EncryptedModule` will run on the server side. This part of the model running in FHE we need to quantize it, thus why we leverage Brevitas for Quantization Aware Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bb6a59-1002-4744-bfe7-baef97a4c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNV(num_classes=10, weight_bit_width=2, act_bit_width=2, in_bit_width=3, in_ch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea920f40-82e1-4c00-8c65-035de4b11d78",
   "metadata": {},
   "source": [
    "We won't be training the model is this notebook as it would be quite computationnaly intensive but we provide an already trained model that satisfies the 8-bit accumulator size constraint and that performs better than random on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cd2622-b0f4-4632-8ff7-8d17a82ac812",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load(\"./8_bit_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476ddd8e-bb04-4d42-844c-cde14b5817e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(loaded[\"model_state_dict\"])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e7ce1e-c287-48e5-ae6d-29df15a12110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: .data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           ), Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: .data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           ))\n"
     ]
    }
   ],
   "source": [
    "IMAGE_TRANSFORM = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=\".data/\",\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=IMAGE_TRANSFORM,\n",
    "        target_transform=None,\n",
    "    )\n",
    "except RuntimeError:\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=\".data/\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=IMAGE_TRANSFORM,\n",
    "        target_transform=None,\n",
    "    )\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root=\".data/\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=IMAGE_TRANSFORM,\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "print((train_set, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c9182-ae7b-47f3-b810-d623bddad1dc",
   "metadata": {},
   "source": [
    "We use a sub-sample of the training set for the FHE compilation to maintain acceptable compilation times and avoid out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9de82bb-a05d-4b34-b6ed-15ffe75014ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "train_sub_set = torch.stack(\n",
    "    [train_set[index][0] for index in range(min(num_samples, len(train_set)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841a675-185c-4723-9c8f-a1bcc9e17cf2",
   "metadata": {},
   "source": [
    "Since we will be compiling only a part of the network we need to give it representative inputs, in our case the first feature map of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ed8dfb-5f5c-4dad-ba2b-6bc74dfccc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing -> images -> feature maps\n",
    "with torch.no_grad():\n",
    "    train_features_sub_set = model.clear_module(train_sub_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93f390-88d6-41a5-acf4-67dc42b80fe4",
   "metadata": {},
   "source": [
    "# Virtual Library compilation\n",
    "\n",
    "In a first time we can make sure that our FHE constraints are respected using the virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2af71c-baba-4edb-8cc3-f3795ae31313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation finished in 175.60 seconds\n",
      "Max bitwidth: 8 bits -> it works in FHE!!\n"
     ]
    }
   ],
   "source": [
    "optional_kwargs = {}\n",
    "optional_kwargs[\"configuration\"] = Configuration(\n",
    "    dump_artifacts_on_unexpected_failures=True,\n",
    "    enable_unsafe_features=True,  # This is for our tests in Virtual Library only\n",
    "    show_graph=True,\n",
    "    show_mlir=False,\n",
    "    show_optimizer=True,\n",
    ")\n",
    "optional_kwargs[\"use_virtual_lib\"] = True\n",
    "\n",
    "# Compile the model\n",
    "compilation_onnx_path = \"compilation_model.onnx\"\n",
    "print(\"Compiling the model\")\n",
    "start_compile = time.time()\n",
    "quantized_numpy_module = compile_brevitas_qat_model(\n",
    "    # our encrypted model\n",
    "    torch_model=model.encrypted_module,\n",
    "    # a representative inputset to be used for compilation\n",
    "    torch_inputset=train_features_sub_set,\n",
    "    **optional_kwargs,\n",
    "    output_onnx_file=compilation_onnx_path,\n",
    ")\n",
    "clear_output()\n",
    "end_compile = time.time()\n",
    "print(f\"Compilation finished in {end_compile - start_compile:.2f} seconds\")\n",
    "\n",
    "# Check that the network is compatible with FHE constraints\n",
    "assert quantized_numpy_module.forward_fhe is not None\n",
    "bitwidth = quantized_numpy_module.forward_fhe.graph.maximum_integer_bit_width()\n",
    "print(\n",
    "    f\"Max bitwidth: {bitwidth} bits -> it works in FHE!!\"\n",
    "    if bitwidth <= 8\n",
    "    else f\"Max bitwidth: {bitwidth} bits too high for FHE computation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cd61d-800d-4424-918f-959ab26c32a3",
   "metadata": {},
   "source": [
    "## Virtual library inference\n",
    "\n",
    "A good sanity check is also to see that the whole pipeline returns what is expected (with a certain error).\n",
    "\n",
    "To go from the following code to the FHE version we only need to add the encrypt/decrypt part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114f4fad-9f9a-4006-9337-6a0ee7a1c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = train_set[0]\n",
    "with torch.no_grad():\n",
    "    feature_maps = model.clear_module(img[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1e3763-9925-4537-94e1-bb6d32bd41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_feature_maps = quantized_numpy_module.quantize_input(feature_maps.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83004547-f0d8-49e1-aa25-842c4f5458d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_output = quantized_numpy_module.fhe_circuit.simulate(quantized_feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd05bf7-28a2-4bfd-80dd-adabf6eca843",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_output = quantized_numpy_module.dequantize_output(quantized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db4f92d-6092-418f-b5e1-0aba544ffa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch_output = model(img[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c3aa51-fb4d-4da3-9564-6be636b42799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0171,  0.0171, -0.0215,  0.0122,  0.0232, -0.0144,  0.0042, -0.0115,\n",
      "          0.0180,  0.0065]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch_output - vl_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398cdb2-b9aa-4937-97c2-0e5c41d1ac00",
   "metadata": {},
   "source": [
    "We see that we have have some difference between the torch model output and the virtual library.\n",
    "\n",
    "This is expected but as we can see in the following code blocks we have no difference in top-k accuracies between Pytorch and the Virtual Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c34ffdb-6aca-4464-b26d-bb12c4afb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(file_path: str, k=3):\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    prob_columns = [elt for elt in predictions.columns if elt.endswith(\"_prob\")]\n",
    "    predictions[\"pred_label\"] = predictions[prob_columns].values.argmax(axis=1)\n",
    "\n",
    "    # Equivalent to top-1-accuracy\n",
    "    for k_ in range(1, k + 1):\n",
    "        print(\n",
    "            f\"top-{k}-accuracy: \",\n",
    "            top_k_accuracy_score(\n",
    "                y_true=predictions[\"label\"], y_score=predictions[prob_columns], k=k_\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732267b9-c7cb-42ee-a40a-b21337d41430",
   "metadata": {},
   "source": [
    "We can use the `infer_vl.py` script to generate the predictions of the model using Pytorch for the first layer and the Virtual Library for the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5918f4d-91dc-434a-90c8-baf79e13f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1-accuracy:  0.6234\n",
      "top-2-accuracy:  0.8075\n",
      "top-3-accuracy:  0.8905\n"
     ]
    }
   ],
   "source": [
    "%run infer_vl.py\n",
    "clear_output()\n",
    "evaluate(\"./predictions_vl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee3362-62c0-4e4a-a4c2-4af63710e7dc",
   "metadata": {},
   "source": [
    "And the `infer.py` script to generate the pure Pytorch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65849856-cf80-4b93-99c1-56deeac83c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1-accuracy:  0.6234\n",
      "top-2-accuracy:  0.8075\n",
      "top-3-accuracy:  0.8905\n"
     ]
    }
   ],
   "source": [
    "%run infer.py\n",
    "clear_output()\n",
    "evaluate(\"./predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a2eb9-7e4f-4890-b8c5-71765bb8bb18",
   "metadata": {},
   "source": [
    "# FHE compilation\n",
    "\n",
    "Now that we checked that we satisfied our constraints we can procede with the FHE compilation that will generate a FHE circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a267dd4e-b9b4-4428-893c-362eb87e2f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation finished in 177.73 seconds\n"
     ]
    }
   ],
   "source": [
    "configuration = Configuration(\n",
    "    show_graph=False,\n",
    "    show_mlir=False,\n",
    "    show_optimizer=True,\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "compilation_onnx_path = \"compilation_model.onnx\"\n",
    "print(\"Compiling the model\")\n",
    "start_compile = time.time()\n",
    "\n",
    "quantized_numpy_module = compile_brevitas_qat_model(\n",
    "    # our encrypted model\n",
    "    torch_model=model.encrypted_module,\n",
    "    # a representative inputset to be used for both quantization and compilation\n",
    "    torch_inputset=train_features_sub_set,\n",
    "    configuration=configuration,\n",
    "    global_p_error=0.15,\n",
    "    output_onnx_file=compilation_onnx_path,\n",
    ")\n",
    "clear_output()\n",
    "end_compile = time.time()\n",
    "print(f\"Compilation finished in {end_compile - start_compile:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e373a3e-d84e-4005-979a-5bd58455e5e7",
   "metadata": {},
   "source": [
    "In this notebook we showed how to compile a splitted VGG model trained to classify CIFAR-10 images in FHE.\n",
    "\n",
    "While satisfying the FHE constraints the model achieves the following performances:\n",
    "\n",
    "- top-1-accuracy: 0.6234\n",
    "- top-2-accuracy: 0.8075\n",
    "- top-3-accuracy: 0.8905\n",
    "\n",
    "We don't launch the inference in FHE in this notebook as it takes quite some time just to infer on one image.\n",
    "\n",
    "For reference we ran the inference of one image on an AWS c6i.metal compute machine, using the `fhe_inference.py` script, and got the following timings:\n",
    "\n",
    "- Time to compile: 103 seconds\n",
    "- Time to keygen: 639 seconds\n",
    "- Time to infer: 37706 seconds (more than 10 hours)"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
